\chapter{Base Algorithms}
The algorithms that I investigated for this thesis can be grouped very broadly into two approaches, which I will describe in the following two sections.

\section{Enumeration algorithms}
\subsection{Model Enumeration}
A simple definition of the backbone is that it is the intersection of all models of it's formula. If a literal is not part of the backbone, there must exist a model that contains the negation of that literal. Therefore if we had a way to iterate over every single model of the formula and, starting with the set of both literals for every variable and removing every literal from that set that was missing in one of these models, that set would end up being the backbone of the formula. \cite{mjl10} as well as \cite{mjl15} list an algorithm that does exactly this. 

\begin{algorithm} %ueber alle models iterieren
\caption{{\sc Enumeration-based backbone computation}}
\DontPrintSemicolon
\KwIn{A satisfiable formula $F$}
\KwOut{Backbone of $F$, $\nu_r$}
$\nu_r \gets \{ x | x \in Var(F) \} \cup \{ \neg x | x \in Var(F) \}$\;
\While{$\nu_r \neq \emptyset$}{
	$(outc,\nu) \gets SAT(F)$\;
	\If{$outc = \bot$}{
		$return \; \nu_r$
	}
	$\nu_r \gets \nu_r \cap \nu$\;
	$\omega_B \gets \bigvee_{l \in \nu}\neg l$\;
	$F \gets F \cup \omega_B$\;
}
%$assert(\nu_r = \emptyset)$\;
$\Return\; \nu_r$\;
\end{algorithm}

Here, found models are prevented from being found again by adding a blocking clause of said model and the algorithm terminates once all models are prohibited and the formula became unsatisfiable through this.

\subsection{Upper Bound Reduction}

Clearly, calculating every single model of a formula leaves room for optimization. Most models of a common boolean formula differ by small, independent differences that can just as well occur in other models. Therefore the intersection of only a handful of models can suffice to result in the backbone, as long as these models are chosen to be as different as possible. This was achieved in \cite{mjl15} as is described in algorithm 2.

\begin{algorithm}
\caption{{\sc Iterative algorithm with complement of backbone estimate}}
\DontPrintSemicolon
\KwIn{A satisfiable formula $F$}
\KwOut{Backbone of $F$, $\nu_r$}
$(outc,\nu_r) \gets SAT(F)$\;
\While{$\nu_r \neq \emptyset$}{
	$bc \gets \bigvee _{l\in\nu_r}\neg l$\;
	$(outc,\nu) \gets SAT(F \cup \{bc\})$\;
	\If{$outc = \bot$}{
		$return\; \nu_r$\;
	}
	$\nu_r \gets \nu_r \cap \nu$\;
}
$\Return \; \nu_r$\;
\end{algorithm}

It generates an upper bound $\nu_r$ of the backbone by intersecting found models and inhibits this upper bound instead of individual models. This blocking clause is much more powerful, because it enforces not only that a new model is found, but also that this new model will reduce the upper bound estimation of the backbone in each iteration. 

This is because what remains after the intersection of a handful of models, are the assignments that were the same in all these models and from that we make a blocking clause that prohibits the next model to contain that particular combination of assignments. The next model will then have to be different from all previous models for at least one of the variables in the blocking clause to satisfy it.

Eventually $\nu_r$ will be reduced to the backbone. This can be easily recognized, because the blocking clause of the backbone or any of it's subsets makes the formula unsatisfiable, except in the case that the formulas backbone would be empty. 

Note that it is not particularly important for the algorithm whether the blocking clauses remain in $F$ or get replaced by the next blocking clause, because the new blocking clause $bc_{i+1}$ always subsumes the previous one $bc_i$, meaning that every solution that is prohibited by $bc_{i+1}$ is also prohibited by $bc_i$ and $F \cup \{bc_i , bc_{i+1}\}$ is equivalent to $F \cup \{bc_{i+1}\}$ concerning the set of models.

This algorithm is implemented in the Sat4J library under the designation $IBB$. 

\subsection{Preferences}

This approach still leaves much of it's efficiency to chance. Theoretically the solver might return models with only the slightest differences from each other, when other models could reduce the set of backbone candidates much more. For example the blocking clause can be satisfied with only one literal in it being satisfied, but if we were to find a model that satisfies all literals in the blocking clause, we can immediately tell that the backbone is empty and we would be finished. So it would be a good approach for backbone computation if we could direct our sat solver to generate models that disprove as many of the literals in the blocking clause as possible. Precisely this has been described by \cite{PJ18}, but has also been proposed much earlier by \cite{kk01}. 

\cite{PJ18} describe an algorithm called $BB-PREF$ or $Prefbones$, which makes use of a slightly modified SAT solver based on CDCL, which is called $prefSAT$ in the algorithm below. It can be configured with a set of preferred literals $prefs$. Typically, when the CDCL algorithm reaches the point where it has the freedom to decide the assignment of a variable, it consults a heuristic that tries to predict the best choice of variable and assignment to reach a model, so to speak, trying to predict assignments in the model that it tries to find. Instead, $prefSAT$ uses two separate instances of these heuristics $h_{pref}$ and $h_{tail}$, which by themselves may work just as the single heuristic used in the ordinary CDCL solver. The key difference in $prefSAT$ is, that $h_{pref}$, which contains the literals in $prefs$,  is consulted first for decisions, and only when all variables with a preferred assignment are already assigned, $h_{tail}$ is used to pick the most important literal, which only contains literals that are not preferred.


\begin{algorithm}
\caption{{\sc BB-pref: Backbone computation using pref-SAT}}
\DontPrintSemicolon
\KwIn{A satisfiable formula $F$ in CNF}
\KwOut{All literals of the backbone of $F$, $\nu_r$}

$(\_,\nu_r) \gets SAT(F) $\;
%$setPreferences(\{\neg l : l \in \nu_r\})$\;

\InfiniteLoop{}{
	$prefs \gets \{\neg l : l \in \nu_r\}$\;
	$(\_,\nu) \gets prefSAT(F,prefs)$\;
	\If{$\nu \supseteq \nu_r $ }{
		$\Return\; \nu_r$\tcp*{No preference was applied}
	}
	%$removePreferences(\{\neg l : l \in \nu_r \textbackslash \nu\})$\;

	$\nu_r \gets \nu_r \cap \nu$\;
}
\end{algorithm}

This algorithm also differs from $IBB$ in that it does not add blocking clauses, and that is also why it cannot use the case when $F$ becomes unsatisfiable to terminate the algorithm. Instead it relies on the preferences to be taken into account. Except for the case where a formula has only one model, CDCL must make at least one decision. That decision must come from $h_{pref}$, except for the case that CDCL learned axiomatic assignments for all variables in $prefs$. Depending on whether the learned value for the variables in $prefs$ contradicts all preferences it may take another call to $prefSAT$, but at the latest then no more changes will happen to $\nu_r$ and the algorithm terminates. The return condition also covers the case when the backbone turns out to be empty, because then $\nu_r$ was reduced to $\emptyset$ and that is a subset of every set.

Note that the algorithm was written slightly different from what is listed in \cite{PJ18} to make the relation with common enumeration algorithms more apparent and also make it easier to read.

The return condition makes this algorithm inflexible, as the preferences have to be taken into account without exception. If not, a model might be returned that terminates the algorithm prematurely, because it did not properly test a variable assignment, instead taking a shortcut to save time in the calculation of a model. Since the purpose of this thesis was to experiment with solvers and we were interested in the concrete effects of preferences by themselves on the backbone computation, we created a variation of Prefbones, that uses the previous approach of upper bound reduction, adding a blocking clause to $F$ in every iteration and terminating when $F$ would become unsatisfiable. This made the preferences algorithmically completely optional and allowed to experiment with many variations on the concept.

\begin{algorithm}
\caption{{\sc BB-pref: Backbone computation using pref-SAT and blocking clause}}
\DontPrintSemicolon
\KwIn{A formula $F$ in CNF}
\KwOut{All literals of the backbone of $F$, $\nu_r$}

$(\_,\nu_r) \gets SAT(F)$\;
%$setPreferences(\{\neg l : l \in \nu_r\})$\;

\InfiniteLoop{}{
	$bc \gets \bigvee _{l\in\nu_r}\neg l$\;
	$F \gets F \cup \{bc\}$\;
	$prefs \gets \{\neg l : l \in \nu_r\}$\;
	$(outc,\nu) \gets prefSAT(F,prefs)$\;
	\If{$outc = \bot$}{
		$\Return\; \nu_r$\;
	}
	%$removePreferences(\{\neg l : l \in \nu_r \textbackslash \nu\})$\;
	$\nu_r \gets \nu_r \cap \nu$\;
}
\end{algorithm}


\section{Iterative algorithms}
\subsection{Testing every literal}
Alternatively, you can define the backbone as all literals that occur with the same assignment in all models of it's problem, which implies that enforcing that variable to it's negation should make the formula unsatisfiable. This definition already leads to a simple algorithm that can calculate the backbone, by checking both assignments of every literal for whether it would make the formula unsatisfiable, see Algorithm 1. This algorithm is referenced in \cite{mjl10}
\begin{algorithm}
\caption{{\sc Iterative algorithm (two tests per variable)}}
\DontPrintSemicolon
\KwIn{A satisfiable formula $F$ in CNF}
\KwOut{All literals of the backbone of $F$ $\nu_r$}
$\nu_r \gets \emptyset$\;
\For{$x \in Var(F)$}{
	$(outc_1,\_) \gets SAT(F \cup \{x\})$\;
	$(outc_2,\_) \gets SAT(F \cup \{\neg x\})$\;
	$assert(outc_1 = \top \vee outc_2 = \top)$ \tcp*{Otherwise F would be unsatisfiable}
%	\If{$outc_1 = \bot \wedge outc_2 = \bot$}{
%		$return\; \emptyset$\; 	}
	\ElseIf{$outc_1 = \bot$}{
		$\nu_r = \nu_r \cup \{\neg x\}$\;
		$F = F \cup \{\neg x\}$\;
	}
	\ElseIf{$outc_2 = \bot$}{
		$\nu_r = \nu_r \cup \{x\}$\;
		$F = F \cup \{x\}$\;
	}
	
}
\Return{$\nu_r$}\;
\end{algorithm}

As is commonly written in literature about boolean satisfiability, the two calls to the $SAT$ function return a pair which consists first of whether the given function was satisfiable at all and, secondly, the found model, which in this case is discarded. There is no good algorithm that can tell whether a boolean formula is satisfiable or not without trying to find a model for said formula, but we can use it to greatly improve the algorithm above by combining this approach with that of the enumeration algorithms.

\subsection{Combining with Enumeration}
First observe that any model of $F$ would already reduce the set of literals to test by half, because for every assignment missing in the model, we know that it cannot be part of the backbone, so there is no need to test it. 

This can be repeated with every further model that we find. The following algorithm is another one that is listed in both \cite{mjl10} and \cite{mjl15} and is implemented in the Sat4J library as $BB$

\begin{algorithm}
\caption{{\sc Iterative algorithm (one test per variable)}}
\DontPrintSemicolon
\KwIn{A satisfiable formula $F$ in CNF}
\KwOut{All literals of the backbone of $F$ $\nu_r$}

$(outc,\nu) \gets SAT(F)$\;
$\Lambda \gets \nu$\;
$\nu_r \gets \emptyset$\;
\While{$\Lambda \neq \emptyset$}{
	$l \gets pick\; any\; literal\; from\; \Lambda$\;
	$(outc,\nu) \gets SAT(F \cup \{\neg l\})$\;
	\If{$outc = \bot$}{
		$\nu_r \gets \nu_r \cup \{l\}$\;
		$\Lambda \gets \Lambda \textbackslash \{l\} $\;
		$F \gets F \cup \{l\}$\;
	}
	\Else{
		$\Lambda \gets \Lambda \cap \nu$\;
	}
	
}
$\Return \; \nu_r$\;
\end{algorithm}

Note that both possible results of the call to the sat solver are converted to useful information. In the else branch, the formula together with the blocked literal $l$ was still solvable. In this case $\nu$ is still a valid model for $F$, so we can search through it to look for more variables that don't need to be checked. Note that here $\nu$ must contain $\neg l$, as it was enforced.

In the other case, we identified $l$ as a backbone literal. In that case it will be added to the returned set, removed from the set of literals to test and, lastly, added to the problem $F$, which increases performance in subsequent solving steps. However it would be even better, not only to reuse the learned backbone literals, but all learned clauses.

\subsubsection{Hidden disadvantage}

You would think that the iterative approach is just an improvement over the model enumeration algorithm. However it has a hidden disadvantage over them, which stems from how the given problem $F$ is modified as part of the algorithm. Remember how we wrote in chapter 2.1.2 how it does not particularly matter whether the blocking clause is removed or stays. Actually it can make a slight difference to the performance of the individual sat solving calls. The CDCL algorithm develops learned clauses during it's runtime. These learned clauses do not change the set of models for the formula but make it easier to avoid partial assignments of variables that by themselves make the formula unsatisfiable 

TODO beispiel, mindestens 4 variablen 

Clause learning in CDCL happens through a process called resolvent building. Here, two contradicting assignments are evaluated for their reasons, eventually ending up with a set of variable assignment decisions that led to the contradiction which is subsequently prohibited by a learned blocking clause.

TODO resolventen algorithmus \newline
TODO resolventen bildchen 

These learned clauses can be reused in subsequent sat calls and can improve the speed of the solver dramatically. They prevent contradictory variable assignments, specifically those that the sat solver ran into before and have a good chance to be made again if nothing prevents the solver from it. 

However reusing learned clauses is not as easy for the $BB$ solver as for the $IBB$ solver. Learned clauses are based on a subset of the clauses in $F$. Their existence is virtual so to speak, implied through these base clauses, just not easy to recognize. Adding more clauses to the formula does not remove a learned clause, as the set of base clauses is untouched. At most it might be possible to subsume it with another learned clause. However if a clause is removed from $F$, it might be that the learned clause is no longer implied through the formula, therefore making it invalid. 


Example: Using the iterative approach on formula $F = \{(a,b,c)\}$ together with blocking clause $(\neg a)$ implies the clause $(b,c)$. CDCL would learn this clause if it were configured to assign $\bot$ in every decision and assign $\top$ only through unit implications. This learned clause must be discarded. Otherwise when we test with the blocking clause $(\neg b)$, together with $(b,c)$ it would imply $(c)$, making $c$ a backbone of $F$, which it clearly isn't.


In the $BB$ solver, except for the very first solving step, no formula that is solved is a subset of any other, because every time a single different literal is enforced. Therefore the clauses that are learned in these solving steps must be discarded and cannot help the other solving steps \footnote{
The most you could do would be to keep track which clauses went into the generation of each learned clause and then only discard those learned clauses that were completely unrelated from the clause that was deleted from $F$. And that might have to be done recursively, if one of the base clauses is itself a learned clause. The SAT4J framework simply discards all learned clauses of the formula when a clause is removed from it.}. 
The $IBB$ solver on the other hand only adds further constraints, therefore it can reuse all it's learned clauses and speed up the model generation over time.

NOPE, alles falsch. BB behält in praxis gelernte klauseln. Einziger nachteil ist, dass unsat ergebnisse kein model zurückgeben und dadurch nicht die kandidaten schneiden