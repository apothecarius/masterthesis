\section{Cheap identification of backbone literals}
This section describes various ways that allow you to recognize backbone literals without an additional satisfiability check. Knowing these backbone literals early can speed up the individual calls to the SAT solver, because enforcing the backbone literals prevents the solver from trying to find solutions containing the negation of backbone literals, which by definition don't exist. Specifically for algorithms that apply preferences, knowing backbone literals explicitely can be helpful, because then you can remove some of those preferences that go against the backbone and could never be satisfied anyway.

%Furthermore the latter two subsections here work solely on the set of already identified backbone literals.


\subsection{Learned literals}
\label{subsec:axiomatic}
The most straightforward method to quickly identify backbone literals is to scan the formula for clauses with only a single literal. Since these clauses have only one possible way to become satisfied, that assignment must be used in every model of the formula and is therefore backbone.

Additionally you should check the CDCL table that your solver creates. If you look at this table you might find variable assignments through unit implication which happened before any decision. This includes all assignments from the paragraph above, but also those that are implied through these\footnote{Example: Formula $\{\{a\},\{\neg a, b\}\}$ has the backbone $\{a,b\}$, but only $a$ is immediately obvious.}.

It makes sense to do this check after every SAT computation, as every different way that leads to a different model brings different learned clauses, that may sometimes consist of a single literal. The number of these clauses depends on the structure of your formula. If it is easy to solve, very few conflicts will occur and in turn, very few clauses will be learned.

An expansion on this would be to look for pairs of clauses $(a,b)$, $(a,\neg b)$ for any two variables of the formula. The only way to fulfill this pair of clauses is to assign $a$ to $\top$\footnote{Which fits the resolution formula described in 
section \ref{ss:cdcl}}. This scheme can theoretically be applied to any clause size, but then you would require a quadratically increasing number of clauses to determine a backbone which would first increase computation time and secondly decrease the chance that the necessary set of clauses was available.

However, if you employ a backbone algorithm that adds clauses during its course, then learned literals may not be guaranteed to be valid backbone literals, just as learned clauses may not be valid beyond the running backbone computation\footnote{
	A simple example: You find an implicant $\{a\}$ with only a single literal. Then you add the blocking clause $\{\neg a\}$. But in $F \cup \{\neg a\}$ $\neg a$ is suddenly a backbone literal.}.
In theory, you could track for each learned clause which original clauses were used to create it. This way, you could tell for each individual learned clause (and literal), whether it would also be valid for the original formula. However, as far I investigated it, $Sat4J$ does not support this feature. This means that the $PB1$ and $IBB$ algorithms should not be combined with this optimization, except for their very first SAT call since it is done before a blocking clause is added. It also means that the same algorithms have to discard all their learned literals and clauses together with the inserted blocking clauses before they return.

The other algorithms, $BB$ and $PB0$ on the other hand do not have this issue. $PB0$ does not modify the formula at all and the $BB$ algorithm can use a different mechanism for its temporary modifications which is to solve it under $assumptions$. This means that you start the SAT computation by putting decisions that correspond to the assignments that you want to enforce at the very beginning of the CDCL table. Then once you have to backtrack these decisions, you know that your formula is unsatisfiable with your assumptions. The difference that this makes regarding learned clauses is that the content of them will differ. When implemented as temporary clauses, your assumption is taken as ground truth, as part of your formula. The resolution step for the variable associated with the assumption will remove it from the learned clause. Without the clause that implemented the assumption, that learned clause would be too strong.\\
However as decisions, where this assumption is involved with a conflict a corresponding literal will simply be added to the learned clause. That way the learned clause will later only come into effect under the circumstance that the variable that has now an assumption is then assigned the same value.

For this reason, both $BB$ and $PB0$ are also able to keep their learned clauses after a backbone computation, even under assumptions. This will become relevant later in the $Results$ section.


%TODO was ist mit gelerntem auf basis von assumptions
%\\TODO experBB results dazunehmen

\iffalse

However, if you want to use this feature in each iteration, make sure that your learned clauses do not stem from the means in which you enforce different models.

 for example the blocking clauses in $IBB$ or the checked literal in $BB$. Otherwise, you might confuse the backbone literals of your original problem $F$ with literals that only became axiomatic in your modified formula. 

The $BB$ solver manages this by having the feature of $assumptions$, which were first proposed in \cite{ENSO03}. These assumptions are assignments that are tested for whether the formula has a model that contains these assignments. For this, the CDCL table is first filled with decisions, that match the assumptions and subsequently the CDCL algorithm runs its course ordinarily, except that the root decision level is after the entries that assign the assumptions. This has the effect that the assignments from the assumptions can not be taken back through backtracking, instead returning that $F$ is unsatisfiable under the given assumptions.

Once a model was found, all the learned clauses are checked whether they are related to the assumptions, and if so, they must be discarded, as is mentioned in \cite{WKS01}. This way, the assumptions will not have any long term effect
\footnote{\cite{ENSO03} actually states that ``all learned clauses are safe to keep''. \cite{WKS01} differs from \cite{ENSO03} in that here clauses are removed explicitely whereas assumptions are removed automatically.}.


Note that there are cases where you want to keep learned clauses between SAT calls. For example you can pass the solver the set of literals of which you already know that they are part of the backbone to help the solving process. Clauses that are learned from these learned literals in combination with other clauses of the formula will also help in this, so they should also be kept. In this case, for each identified backbone literal $l_b$ augment your formula $F$ to $F \cup \{l_b\}$, which is actually fully equivalent to $F$ in terms of the set of it's models, just more explicit in the fact that $l_b$ is a backbone of it.


Additionally, you can identify axiomatic literals if you check the table that is generated by CDCL when you search for a model for $F$. If it lists assignments that happened through unit propagation before any decision happened, then you can safely assume that they are axiomatic and part of the backbone, except of course if they were implied by modifications of F.

TODO tabelle mit wieviele literale dabei rausgefunden werden

\fi
\subsection{Unit implication}
\label{subsec:unitImpl}

When you have some backbone literals identified, there are some methods that you can apply on this set, which can potentially expand it without a complete model calculation. 

One method becomes obvious, if you recall the CDCL algorithm, specifically unit propagation. Suppose you have a clause where all but one of its variables turned out to be part of the backbone, however all of them with the exact opposite sign from that in the clause, so that the clause is still not satisfied by them. In this case you are forced to assign the remaining literal in a way that it does satisfy the clause and you have to do this in every possible model, since what forces you to do that are backbone literals. Therefore, this unit implied assignment is in the backbone.

An efficient algorithm for this would be as follows: Keep a counter for each of the clauses in your formula that indicates the number of not satisfying backbone literals in said clause. When you identify a new backbone literal, increment all the counters where that literal occurs in negation. You can drop the counter completely for clauses where the newly identified backbone literal occurs with the same sign so that it satisfies the clause\footnote{
	In fact, you could remove the whole clause from your formula, since it will always be satisfied through the backbone literal. You could say, it get's subsumed by a clause with only the backbone literal.}.
If the counter reaches the length of its clause minus one, then you know that to satisfy this particular clause, the remaining literal in it has to be in the backbone. In this case you can compare its literals with your current inventory of backbone literals to identify the remaining one. This algorithm should be done inbetween every SAT computation with only those backbone literals that were identified in the last iteration.


Without an efficient implementation, this search for unit implied backbone literals might consume a lot of time. If you try to find the backbone of a formula where the computation takes many SAT calls which each return relatively quickly, the time spent to search for unit implied backbone literals can actually exceed the time spent in the SAT solver if it's implemented inefficiently.

I have tested this method in two solvers, $BB$ and a variant of $PB1$ that identifies backbone literals only through the learned literals\footnote{
	In the previous section I stated, that the $PB1$ solver should not be combined with learning backbone literals, but I only realized this constraint much later. During testing, the computed backbones were always verified and an incorrectly identified backbone literal never occurred. The subsequent observations are still valid, independently of this.}.
The tested files were from a SAT competition. Here it showed, that for the method described in this section it is important to supply a sufficient number of already known backbone literals for it to have a positive impact on runtime. The learned backbone literals alone could rarely supply this, before the $PrefBones$ algorithm terminated from other conditions. However the iterative approach of the $BB$ solver, testing every yet unidentified literal individually, was much faster at providing positively identified backbone literals that you would need to imply other backbone literals through unit implication.

%TODO tabelle mit unit implikation von pb1e hervorzaubern

For further investigation on the effects of this method see section \ref{ss:result_unit}.

%This clearly showed that this method requires a critical mass of already known backbone literals to have a benefit, because as already stated, if there even exists a clause in $F$ where this method can be applied, you need to know the $n_c - 1$ other backbone literals in $c$ beforehand.





\subsection{Implication through cooccurrence}
\label{subsec:coocc}

%\begin{wraptable}{r}{7cm}
Another method to recognize backbone literals from other ones is described in \cite{wbxcl16} and the rationale goes as follows:

%\newtheorem{Printed output}
\begin{lemma}
Given a backbone literal $a$ and another literal $b$. If $b$ occurs in all clauses that also contain $a$, then $\neg b$ must be part of the backbone. 

Proof: Assuming $\neg b$ was not in the backbone, then there would have to exist a model that contained $b$. Given that all clauses that contain $b$ also contain $a$, $a$ cannot be part of the backbone.
\end{lemma}

\begin{wraptable}[31]{l}{8cm}
%\begin{wraptable}[31]{l}{8cm} % first optional parameter defines height of wraptable to avoid too much vertical space to be eaten away.
\begin{tabular}{l| c c c }
File& $n_{unit}$ & $n_{coocc}$ & $n_{BB}$\\
\hline
dimacs-hanoi5.cnf & 1465 & 47 & 1931 \\
grieu-vmpc-s05-25.cnf & 565 & 0 & 625 \\
grieu-vmpc-s05-27.cnf & 142 & 0 & 677 \\
fla-barthel-200-2.cnf & 33 & 0 & 78 \\
fla-barthel-200-3.cnf & 43 & 0 & 97 \\
fla-barthel-220-1.cnf & 149 & 0 & 184 \\
fla-barthel-220-2.cnf & 0 & 0 & 4 \\
fla-barthel-220-4.cnf & 0 & 0 & 6 \\
fla-barthel-240-2.cnf & 61 & 0 & 128 \\
%%%%%%%%%%%
fla-qhid-280-1.cnf & 1 & 0 & 15 \\
fla-qhid-280-3.cnf & 251 & 2 & 274 \\
fla-qhid-300-1.cnf & 260 & 0 & 291 \\
fla-qhid-300-4.cnf & 270 & 0 & 293 \\
fla-qhid-320-1.cnf & 296 & 0 & 318 \\
fla-qhid-320-2.cnf & 0 & 0 & 2 \\
fla-qhid-320-5.cnf & 283 & 2 & 312 \\
fla-qhid-340-2.cnf & 310 & 0 & 332 \\
fla-qhid-340-3.cnf & 309 & 3 & 333 \\
fla-qhid-340-4.cnf & 301 & 4 & 335 \\
fla-qhid-360-1.cnf & 326 & 0 & 335 \\
fla-qhid-360-5.cnf & 321 & 0 & 349 \\
fla-qhid-380-1.cnf & 344 & 2 & 372 \\
fla-qhid-400-3.cnf & 366 & 0 & 393 \\
fla-qhid-400-4.cnf & 359 & 0 & 392 \\
smallSatBomb.cnf & 0 & 0 & 9 \\
\end{tabular}
\caption{Comparison of number of backbone literals identified through cooccurrence in comparison to the number identified through unit implication and the overall number of backbone literals in the formula.}
\label{tab:coocBB}
\end{wraptable}

In other words, if we determine a new backbone literal, and all clauses that contain it also contain another literal, then we can add the negation of the latter to our backbone set. 

However this strategy does not seem to be very effective, at least for the benchmarks that I tested it on. Table \ref{tab:coocBB} shows the number of literals that were identified through this method in comparison to those that were identifed through unit implication and the number of variables in the formula. It is probably very rare that a variable should always occur together with another one.

%TODO Thore fragen, ob kapitel so wertvoll und ob die nicht theoretisch subsumiert werden müssten



