\section{Cheap Identification of backbone literals}
This section describes various ways that allow you to recognize backbone literals without an additional satisfiability check. Knowing these backbone literals early can speed up the individual calls to the SAT solver, because enforcing the backbone literals prevents the solver from trying to find solutions containing the negation of backbone literals, which by definition don't exist.

%Furthermore the latter two subsections here work solely on the set of already identified backbone literals.


\subsection{Axiomatic literals}
\label{subsec:axiomatic}
The most straightforward method to quickly identify backbone literals is to scan the formula for clauses with only a single literal. Since these clauses have only one possible way to become satisfied, that assignment must be used in every model of the formula and is therefore backbone.

Additionally you should check the CDCL table that your solver creates. If you look at this table you might find variable assignments through unit implication which happened before any decision. This includes all assignments from the paragraph above, but also those that are implied through these\footnote{Example: Formula $\{\{a\},\{\neg a, b\}\}$ has the backbone $\{a,b\}$, but only $a$ is immediately obvious.}.

It makes sense to do this check after every SAT computation, as every different way that leads to a different model brings different learned clauses, that may sometimes consist of a single literal.

An expansion on this would be to look for pairs of clauses $(a,b)$, $(a,\neg b)$ for any two variables of the formula. The only way to fulfill this pair of clauses is to assign $a$ to $\top$\footnote{Which fits the resolution formula described in 
section \ref{ss:cdcl}}. This scheme can theoretically be applied to any clause size, but then you would require a quadratically increasing number of clauses to determine a backbone which would first increase computation time and secondly decrease the chance that the necessary set of clauses was available.

TODO was ist mit gelerntem auf basis von assumptions
\\TODO experBB results dazunehmen

\iffalse

However, if you want to use this feature in each iteration, make sure that your learned clauses do not stem from the means in which you enforce different models.

 for example the blocking clauses in $IBB$ or the checked literal in $BB$. Otherwise, you might confuse the backbone literals of your original problem $F$ with literals that only became axiomatic in your modified formula. \footnote{A simple example: You find an implicant $\{a\}$ with only a single literal. Then you add the blocking clause $\{\neg a\}$. But in $F \cup \{\neg a\}$ $\neg a$ is suddenly a backbone literal.}

The $BB$ solver manages this by having the feature of $assumptions$, which were first proposed in \cite{ENSO03}. These assumptions are assignments that are tested for whether the formula has a model that contains these assignments. For this, the CDCL table is first filled with decisions, that match the assumptions and subsequently the CDCL algorithm runs it's course ordinarily, except that the root decision level is after the entries that assign the assumptions. This has the effect that the assignments from the assumptions can not be taken back through backtracking, instead returning that $F$ is unsatisfiable under the given assumptions.

Once a model was found, all the learned clauses are checked whether they are related to the assumptions, and if so, they must be discarded, as is mentioned in \cite{WKS01}. This way, the assumptions will not have any long term effect
\footnote{\cite{ENSO03} actually states that ``all learned clauses are safe to keep''. \cite{WKS01} differs from \cite{ENSO03} in that here clauses are removed explicitely whereas assumptions are removed automatically.}.


Note that there are cases where you want to keep learned clauses between SAT calls. For example you can pass the solver the set of literals of which you already know that they are part of the backbone to help the solving process. Clauses that are learned from these learned literals in combination with other clauses of the formula will also help in this, so they should also be kept. In this case, for each identified backbone literal $l_b$ augment your formula $F$ to $F \cup \{l_b\}$, which is actually fully equivalent to $F$ in terms of the set of it's models, just more explicit in the fact that $l_b$ is a backbone of it.


Additionally, you can identify axiomatic literals if you check the table that is generated by CDCL when you search for a model for $F$. If it lists assignments that happened through unit propagation before any decision happened, then you can safely assume that they are axiomatic and part of the backbone, except of course if they were implied by modifications of F.

TODO tabelle mit wieviele literale dabei rausgefunden werden

\fi
\subsection{Unit Implication}
\label{subsec:unitImpl}

When you have some backbone literals identified, there are some methods that you can apply on this set, which can potentially expand it without a complete model calculation. 

One method becomes obvious, if you recall the CDCL algorithm, specifically unit propagation. Suppose you have a clause where all but one of it's variables turned out to be part of the backbone, however all of them with the exact opposite sign from that in the clause, so that the clause is still not satisfied by them. In this case you are forced to assign the remaining literal in a way that it does satisfy the clause and you have to do this in every possible model, since what forces you to do that are backbone literals. Therefore, this unit implied assignment is in the backbone.

An efficient algorithm for this would be as follows: Keep a counter for each of the clauses in your formula that indicates the number of not satisfying backbone literals in said clause. When you identify a new backbone literal, increment all the counters where that literal occurs in negation. You can drop the counter completely for clauses where the newly identified backbone literal occurs with the same sign so that it satisfies the clause\footnote{
	In fact, you could remove the whole clause from your formula, since it will always be satisfied through the backbone literal. You could say, it get's subsumed by a clause with only the backbone literal.}.
If the counter reaches the length of it's clause minus one, then you know that to satisfy this particular clause, the remaining literal in it has to be in the backbone. In this case you can compare it's literals with your current inventory of backbone literals to identify the remaining one. This algorithm should be done inbetween every SAT computation with only those backbone literals that were identified in the last iteration.

\iffalse
fuer den fall dass platz generiert werden muss: den obigen algorithmus nochmal formell aufschreiben.
\begin{algorithm}
\caption{{\sc Iterative algorithm (two tests per variable)}}
\label{alg:iterTwo}
\DontPrintSemicolon
\KwIn{A satisfiable formula $F$ in CNF}
\KwOut{All literals of the backbone of $F$ $\nu_r$}


\Return{$\nu_r$}\;
\end{algorithm}
\fi

Without an efficient implementation, this search for unit implied backbone literals might consume a lot of time. If you try to find the backbone of a formula where the computation takes many SAT calls which each return relatively quickly, the time spent to search for unit implied backbone literals can actually exceed the time spent in the SAT solver if it's implemented inefficiently.

I have tested this method in two solvers, $BB$ and a variant of $PB1$ that identifies backbone literals only through the learned literals. The tested files were from a SAT competition. Here it showed, that for the method described in this section it is important to supply a sufficient number of already known backbone literals for it to have a positive impact on runtime. The learned backbone literals alone could rarely supply this, before the $PrefBones$ algorithm terminated from other conditions. However the iterative approach of the $BB$ solver, testing every yet unidentified literal individually, was much faster at providing positively identified backbone literals that you would need to imply other backbone literals through unit implication.

%TODO tabelle mit unit implikation von pb1e hervorzaubern

For further investigation on this method see section \ref{ss:result_unit}.

%This clearly showed that this method requires a critical mass of already known backbone literals to have a benefit, because as already stated, if there even exists a clause in $F$ where this method can be applied, you need to know the $n_c - 1$ other backbone literals in $c$ beforehand.





\subsection{Implication through Cooccurrence}
\label{subsec:coocc}

%\begin{wraptable}{r}{7cm}
Another method to recognize backbone literals from other ones is described in \cite{wbxcl16} and the rationale goes as follows:

%\newtheorem{Printed output}
\begin{lemma}
Given a backbone literal $a$ and another literal $b$. If $b$ occurs in all clauses that also contain $a$, then $\neg b$ must be part of the backbone. 

Proof: Assuming $\neg b$ was not in the backbone, then there would have to exist a model that contained $b$. Given that all clauses that contain $b$ also contain $a$, $a$ cannot be part of the backbone.
\end{lemma}
\begin{wraptable}{l}{8cm}
%\begin{wraptable}[31]{l}{8cm} % first optional parameter defines height of wraptable to avoid too much vertical space to be eaten away.
\begin{tabular}{l| c c c }
File& $n_{unit}$ & $n_{coocc}$ & $n_{var}$\\
\hline
brock400-2.cnf & 0 & 0 & 400 \\
dimacs-hanoi5.cnf & 1465 & 47 & 1931 \\
grieu-vmpc-s05-25.cnf & 565 & 0 & 625 \\
grieu-vmpc-s05-27.cnf & 142 & 0 & 729 \\
johnson8-2-4.cnf & 0 & 0 & 28 \\
fla-barthel-200-2.cnf & 33 & 0 & 200 \\
fla-barthel-200-3.cnf & 43 & 0 & 200 \\
fla-barthel-220-1.cnf & 149 & 0 & 220 \\
fla-barthel-220-2.cnf & 0 & 0 & 220 \\
fla-barthel-220-4.cnf & 0 & 0 & 220 \\
fla-barthel-240-2.cnf & 61 & 0 & 240 \\
\iffalse
fla-komb-200-3.cnf & 166 & 0 & 200 \\
fla-komb-200-5.cnf & 166 & 0 & 200 \\
fla-komb-220-1.cnf & 190 & 0 & 220 \\
fla-komb-220-3.cnf & 187 & 0 & 220 \\
fla-komb-220-4.cnf & 188 & 0 & 220 \\
fla-komb-220-5.cnf & 179 & 3 & 220 \\
fla-komb-240-2.cnf & 211 & 0 & 240 \\
fla-komb-240-3.cnf & 202 & 0 & 240 \\
fla-komb-240-5.cnf & 215 & 0 & 240 \\
fla-komb-260-1.cnf & 212 & 2 & 260 \\
fla-komb-260-3.cnf & 224 & 2 & 260 \\
fla-komb-260-4.cnf & 213 & 0 & 260 \\
fla-komb-280-1.cnf & 244 & 0 & 280 \\
fla-komb-280-3.cnf & 240 & 0 & 280 \\
fla-komb-280-4.cnf & 245 & 0 & 280 \\
fla-komb-280-5.cnf & 224 & 0 & 280 \\
fla-komb-300-3.cnf & 238 & 1 & 300 \\
fla-komb-300-4.cnf & 256 & 2 & 300 \\
fla-komb-300-5.cnf & 272 & 0 & 300 \\
fla-komb-320-2.cnf & 280 & 0 & 320 \\
fla-komb-320-3.cnf & 257 & 0 & 320 \\
fla-komb-320-5.cnf & 251 & 0 & 320 \\
fla-komb-340-1.cnf & 299 & 0 & 340 \\
fla-komb-340-2.cnf & 279 & 0 & 340 \\
fla-komb-340-3.cnf & 300 & 1 & 340 \\
fla-komb-340-4.cnf & 277 & 0 & 340 \\
fla-komb-340-5.cnf & 290 & 0 & 340 \\
fla-komb-360-1.cnf & 305 & 2 & 360 \\
fla-komb-360-4.cnf & 291 & 0 & 360 \\
fla-komb-360-5.cnf & 319 & 0 & 360 \\
fla-komb-380-2.cnf & 330 & 1 & 380 \\
fla-komb-380-3.cnf & 320 & 2 & 380 \\
fla-komb-380-4.cnf & 322 & 1 & 380 \\
fla-komb-380-5.cnf & 314 & 0 & 380 \\
fla-qhid-200-1.cnf & 185 & 0 & 200 \\
fla-qhid-200-2.cnf & 182 & 0 & 200 \\
fla-qhid-200-4.cnf & 181 & 0 & 200 \\
fla-qhid-200-5.cnf & 183 & 0 & 200 \\
fla-qhid-220-2.cnf & 190 & 0 & 220 \\
fla-qhid-220-3.cnf & 197 & 0 & 220 \\
fla-qhid-220-4.cnf & 197 & 0 & 220 \\
fla-qhid-220-5.cnf & 190 & 0 & 220 \\
fla-qhid-240-3.cnf & 10 & 0 & 240 \\
fla-qhid-240-5.cnf & 0 & 0 & 240 \\
fla-qhid-260-1.cnf & 6 & 0 & 260 \\
fla-qhid-260-2.cnf & 232 & 0 & 260 \\
fla-qhid-280-1.cnf & 1 & 0 & 280 \\
fla-qhid-280-3.cnf & 251 & 2 & 280 \\
\fi
fla-qhid-300-1.cnf & 260 & 0 & 300 \\
fla-qhid-300-4.cnf & 270 & 0 & 300 \\
fla-qhid-320-1.cnf & 296 & 0 & 320 \\
fla-qhid-320-2.cnf & 0 & 0 & 320 \\
fla-qhid-320-5.cnf & 283 & 2 & 320 \\
fla-qhid-340-2.cnf & 310 & 0 & 340 \\
fla-qhid-340-3.cnf & 309 & 3 & 340 \\
fla-qhid-340-4.cnf & 301 & 4 & 340 \\
fla-qhid-360-1.cnf & 326 & 0 & 360 \\
fla-qhid-360-5.cnf & 321 & 0 & 360 \\
fla-qhid-380-1.cnf & 344 & 2 & 380 \\
fla-qhid-400-3.cnf & 366 & 0 & 400 \\
fla-qhid-400-4.cnf & 359 & 0 & 400 \\
smallSatBomb.cnf & 0 & 0 & 264 \\
\end{tabular}
\caption{Comparison of number of backbone literals identified through cooccurrence in comparison to the number identified through unit implication.}
\label{tab:coocBB}
\end{wraptable}

In other words, if we determine a new backbone literal, and all clauses that contain it also contain another literal, then we can add the negation of the latter to our backbone set. 

However this strategy does not seem to be very effective, at least for the benchmark that I tested it on. Table \ref{tab:coocBB} shows the number of literals that were identified through this method in comparison to those that were identifed through unit implication and the number of variables in the formula. It is probably very rare that a variables always occurs together with another

%TODO Thore fragen, ob kapitel so wertvoll und ob die nicht theoretisch subsumiert werden müssten



