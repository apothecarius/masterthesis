
The following chapter elaborates on methods to enhance the algorithms that were described in the previous chapter. Depending on the particular combination of algorithm and enhancement, applying the optimization can be be considered a no brainer. However, experimental results show that this is not true without exception and in individual cases we will give thoughts why that is. Other improvements can only be applied to some algorithms due to the data that is available.

\section{Model Reduction}

The algorithms described in section 2 all boil down to testing for each variable whether there exist two models where one assigns the variable to $\top$ and the other to $\bot$. This section describes two strategies to reduce the model that is returned by the sat solver to a subset that tells us more for the purpose of calculating a backbone. Both methods are essentially about using implicants instead of models.

Having a single implicant $I$ that leaves some variables optional immediately tells us, that every possible combination of assignments of the optional literals can make a model, if we just add the assigned literals in $I$. You could say that an implicant implies a large set of models.

Without any further information, a single implicant $I$ tells us, that every one of the optional variables in it cannot be part of the backbone.

Starting with complete models, implicants can be subsets of other implicants, by removing more and more assignments that are not essential. This way you will eventually reach an implicant where all of it's assignments are required and removing any literal from it, would leave some clause unsatisfied. This would be called a prime implicant $I_\pi$.

\subsection{Prime Implicant}
\label{ss:primeImplicant}
\cite{dflbm13} describes an algorithm that allows to calculate the prime implicant from a model in linear time over the number of literal occurrences in the formula. This algorithm works best if you generate the model of a variable with the CDCL algorithm for multiple reasons.

First, it takes advantage of data structures that you also need in a good implementation of CDCL, namely a lookup from each variable to all clauses that contain either literal of that variable. In CDCL this lookup table improves the performance of unit propagation, because you can check exactly the set of clauses that might be affected by the assignment to determine whether one of the clauses has become exhaustively unsatisfied or implies another assignment\footnote{The unit case}. Here, the lookup is used to determine whether a literal is required in the implicant that you are in the process of generating, by looking for clauses that only contain a single satisfying literal anymore, which then must be part of the prime implicant. You can even reuse the watched literals\footnote{
	These are used in CDCL to determine and store the information on whether a clause is satisfied, unsatisfied, unit or neither of them. This is done by having two pointers that rest on unassigned literals in each clause. In this algorithm you can let these pointers rest on satisfying literals.}
 of the clauses, however you would have to change the way in which the propagate, since you take assignments away instead of adding them.

The second fit with CDCL is that it not only generates a model, but also a table containing information on how that model was generated. This information can be used in this algorithm to reduce the number of literals that need to be checked on whether they are required in the resulting prime implicant. You can quickly generate the input $I_r$ by going through the table generated by CDCL and noting down every assignment that happened through unit propagation. These must be part of the prime implicant because to have been assigned through unit propagation at some point in time there must have been a clause that required that particular assignment. 

The only assignments that you really have to test here are those that were decisions. Additionally, you can avoid many decided assignments if you configure your CDCL SAT solver to stop once the formula is satisfied instead of stopping once every variable was assigned. Once the formula is satisfied, no more assignment is necessary, so all further ones are arbitrary decisions that are not necessary in the implicant. 

It is sufficient to go over the remaining set of assignments only once each. After having determined that a literal is required to satisfy the formula, it cannot become optional later. After all, for this the algorithm would have to add assignments instead of taking them away. And after having discarded a literal from our implicant it cannot become required again. For this to happen we would have to drop a literal from our implicant where that was the last one that satisfied some particular clause. This fits the description of a required literal and we do not drop those.

Since the same model could be calculated in different ways by CDCL depending on the particular order in which the variables in it were assigned values, the partition between decided assignments and unit assignments can be different for the same model. Therefore, this method can give you different prime implicants for the same model depending on the CDCL table that you use. Additionally, the order in which you check the decided literals can also make a difference. 


\begin{algorithm}
\caption{{\sc Base approach to compute a prime implicant}}
\DontPrintSemicolon
\KwIn{A formula $F$, a model $I_m$, $I_r$ containing some required literals in $I_m$}
\KwOut{$I_r$, reduced to a primeimplicant of $F$, being a subset of $I_m$}
\While{$\exists l \in I_m \textbackslash I_r$}{
	\If{$\exists c \in F : Req(I_m,l,c)$}{
		$I_r \gets I_r \cup \{l\}$\;
	}
	\Else{
		$I_m \gets I_m \textbackslash \{l\}$\;
	}
}

\Return{$I_r$}\;

\end{algorithm}

The function $Req(I_m,l,c)$ tells, whether the assignment $l$ in the implicant $I_m$ is required to satisfy c. In other words, is $l$ the only literal in $I_m$ that also occurs in $c$.

\subsection{Rotations}
\label{ss:rot}
There is a model reduction method that is even more powerful than calculating the prime implicant. Even better, the concept is much simpler than calculating the prime implicant. 

Any model of a CNF formula can contain multiple implicants and even prime implicants\footnote
	{As an example imagine any formula with only clauses of size two, where the first literals are disjunct from the second literals. This formula has at least two prime implicants $I_1$ and $I_2$ that you can generate by collecting either the first literal of every clause for $I_1$ or the second literal for $I_2$. Since the literals in $I_1$ are disjunct from those in $I_2$, you can build a model of the combination of $I_1$ and $I_2$.}
, so if we could find out from only a single model $M_0$ of $F$, which of these literals occured in all implicants that that model covered, with each model we could reduce the set of backbone candidates even more than with just one of it's prime implicants.

Doing this is actually pretty simple. Instead of generating a small set of various prime implicants, you check for each assignment $a$ in $M_0$, whether it is required in $M_0$, by checking whether $(M_0 \textbackslash a)$ is still an implicant that satisfies $F$. If so, we know that $a$ is not part of the backbone, because we found an implicant without it. This approach was described in \cite{mjl15}.

You can do this reduction faster if you make use of the lookup table described in the previous section, where each literal is mapped to the set of clauses where it occurs. After all, you already know that $M_0$ satisfies $F$, so it is sufficient to check only the clauses that contain the tested literal $a$, whether they are satisfied in a different way in $M_0 \textbackslash \{a\}$. Unaffected clauses must still be satisfied without $a$.

Additionally, if you happen to use a computation strategy, where you have a set of positively identifed backbone literals (for example an iterative algorithm as described in section \ref{sec:iterative}, or when you make use of axiomatic literals as described in the next section), then of course these don't have to be tested either.

%Whether this gives a performance benefit greatly depends on the structure of the formulas. I essentially used two benchmarks, one academic made of examples from the sat competitions and the other being a real-life example from the automobile industry. The academic one contained much fewer clauses (1.000-2.000) compared to the industrial (around 60.000), however the clauses of the academic benchmark contained three literals each, compared to two in the industrial one.  \\
%TODO individuelle vonThore benchmarks mit individuellen satcomp benchmarks vergleichen

%The difference in performance was extreme. Whereas the academic benchmark spent almost all of it's time in the sat solver, for the industrial benchmark my experimental solver with brute-force model rotation spent around 190 times as much time to rotate literals as it did to calculate the model.

%Clauses with many clauses that contain few literals 
%vonThore: 60k zweier klauseln -> brute force scheiÃŸe
%rest: 1k-2k 3er klauseln -> brute force ok
\begin{algorithm}
\caption{{\sc Literal rotation in models }}
\DontPrintSemicolon
\KwIn{A formula $F$, a model $M$}
\KwOut{R, the required literals of all implicants in M}
$R \gets \emptyset$\;
\For{$a \in M$}{
	$I_a \gets M \textbackslash a$\;
	\If{$\exists c \in F : c\langle I_a \rangle = \bot$}{
		$R \gets R \cup \{a\}$\;
	}
}
\Return{$R$}\;

\end{algorithm}

The performance benefit of this approach depends on the particular example. If the individual models of a formula contain very few primeimplicants or even only one, then looking for rotatable literals might not give a benefit over calculating the prime implicants. However formulas where individual models contain many prime implicants with large differences, the benefit can be extreme. See section \ref{chap:results} for experimental results.


%This gives you the information that we would otherwise generate from all implicants that were contained in M.

