
The following chapter elaborates on methods to enhance the algorithms that were described in the previous chapter. Depending on the particular combination of algorithm and enhancement, applying the optimization can be be considered a no brainer. However, experimental results show that this is not true without exception and in individual cases we will give thoughts why that is. Other improvements can only be applied to some algorthms due to the data that is available.

\section{Model Reduction}

The algorithms described in section 2 all boil down to testing for each variable whether there exist two models where one assigns the variable to $\top$ and the other to $\bot$. This section describes two strategies to reduce the model that is returned by the sat solver to a subset that tells us more for the purpose of calculating a backbone. Both methods are related to the concept of the $implicant$.

An implicant is a set of assignments of the variables in the problem $F$ that still satisfies $F$. The difference to models is, that here it is allowed to leave variables undefined. Let's imagine a formula where every clause contains the literal $a$, amongst other literals. Then $(a)$ would be a simple implicant for this formula, because assigning $a$ to $\top$ is sufficient to satisfy each clause. However there may also be a different implicant that does not contain $a$ at all, satisfying the clauses in a different way. If a variable $v$ does not occur in an implicant $I$, we say that $v$ is optional in $I$.

Having a single implicant $I$ that leaves some variables optional immediately tells us, that every possible combination of assignments of the optional literals can make a model, if we just add the assigned literals in $I$. You could say that an implicant implies a large set of models.

Without any further information, a single implicant $I$ tells us, that every one of the variables that are missing in it cannot be part of the backbone.

Starting with complete models, implicants can be subsets of other implicants, by removing more and more assignments that are not essential. This way you will eventually reach an implicant where all of it's assignments are required and removing any literal from it, would leave some clause unsatisfied. This would be called a prime implicant $I_\pi$.

\subsection{Prime Implicant}
\cite{dflbm13} describes an algorithm that allows to calculate the prime implicant from a model in linear time over the number of literal occurrences in the formula. This algorithm works best if you generate the model of a variable with the CDCL algorithm for multiple reasons.

First, it takes advantage of data structures that you also need in a good implementation of CDCL, namely a lookup from each variable to all clauses that contain either literal of that variable. In CDCL this lookup table improves the performance of unit propagation, because you can check exactly the set of clauses that might be affected by the assignment to determine whether one of the clauses has become exhaustively unsatisfied or implies another assignment. Here, the lookup is used to determine whether a literal is required in the implicant that you are in the process of generating, by looking for clauses that only contain a single satisfying literal anymore, which then must be part of the prime implicant. You can even reuse the watched literals of the clauses, however you would have to change the way in which the propagate, since you take assignments away instead of adding them.

The second fit with CDCL is that CDCL not only generates a model, but also a table containing information how that model was generated. This information can be used in this algorithm because if you know of some assignments that they must be in the implicant (passed as $I_r$), checking them can be skipped here. You can quickly generate this set by going through the table generated by CDCL and noting down every assignment that happened through unit propagation. These must be part of the prime implicant because to have been assigned through unit propagation at some point in time there must have been a clause that required that particular assignment. 

TODO beweisen, dass linear durchzulaufen zu einer primimplikante führt. (dass nichts zweimal geprüft werden muss)

The only assignments that you really have to test here are those that were decisions. Additionally, you can avoid many decided assignments if you configure CDCL to stop once the formula is satisfied instead of stopping once every variable was assigned, because once the formula is satisfied, no more assignment is necessary, so only all further ones must be arbitrary decisions that are not necessary in the implicant. 

\begin{algorithm}
\caption{{\sc Base approach to compute a prime implicant }}
\DontPrintSemicolon
\KwIn{A formula $F$, a model $I_m$, $I_r$ containing some required literals in $I_m$}
\KwOut{$I_r$, reduced to a primeimplicant of $F$, being a subset of $I_m$}
\While{$\exists l \in I_m \textbackslash I_r$}{
	\If{$\exists c \in F : Req(I_m,l,c)$}{
		$I_r \gets I_r \cup \{l\}$\;
	}
	\Else{
		$I_m \gets I_m \textbackslash \{l\}$\;
	}
}

\Return{$I_r$}\;

\end{algorithm}

The function $Req(I_m,l,c)$ tells, whether the assignment $l$ in the implicant $I_m$ is required to satisfy c. In other words, is $l$ the only literal in $I_m$ that also occurs in $c$.

\subsection{Rotations}
There is a model reduction method that is more powerful than calculating the prime implicant. Even better, the concept is much simpler than calculating the prime implicant. 

Any model of a CNF formula can contain multiple implicants and even prime implicants\footnote{As an example imagine any formula with only clauses of size two. This formula has at least two prime implicants $I_1$ and $I_2$ ) that you can generate by collecting either the first literal of every clause for $I_1$ or the second literal for $I_2$. If the literals in $I_1$ are disjunct from those in $I_2$ then you can build a model of the combination of $I_1$ and $I_2$.}
, so if we could find out from only a single model $M_0$ of $F$, which of these literals occured in all implicants that that model covered, with each model we could reduce the set of backbone candidates even more than with just one of it's prime implicants.

Doing this is actually pretty simple. Instead of generating a small set various prime implicants, you check for each assignment $a$ in $M_0$, whether it is required in $M_0$, by checking whether $(M_0 \textbackslash a)$ is still an implicant that satisfies $F$. You can do this faster if you make use of the lookup table described in the previous subsection so that you only need to check affected clauses, since as you start from a known model, every unaffected clause should still be satisfied. However, tests with our implementation showed that the time that it takes to test an implicant by brute force, iterating over each clause and looking for a satisfied literal, was negligible, so there is hardly a benefit in something more complicated.


\begin{algorithm}
\caption{{\sc Base approach to compute a prime implicant }}
\DontPrintSemicolon
\KwIn{A formula $F$, a model $M$}
\KwOut{R, the required literals of all implicants in M}
$R \gets \emptyset$\;
\For{$a \in M$}{
	$I_a \gets M \textbackslash a$\;
	\If{$\exists c \in F : c\langle I_a \rangle = \bot$}{
		$R \gets R \cup \{a\}$\;
	}
}
\Return{$I_a$}\;

\end{algorithm}

This gives you the information that we would otherwise generate from all implicants that were contained in M.

