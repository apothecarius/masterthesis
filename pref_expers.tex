As is mentioned in \cite{PJ18}, the concept of preferences has not experienced much research as of yet. That is why I experimented a lot with various modifications on the concept for the purpose of this thesis.

All of the following modifications were implemented ontop of algorithm \ref{alg:blockPref} ($PB1$). Algorithm \ref{alg:pb0} (PB0) depends on that all available preferences are taken into account strictly, so there is not much space to experiment there without making the algorithm unreliable. 


\section{Forgetting preferences}
During the course of solving a CNF formula with the CDCL algorithm, many assignment conflicts occur which must be resolved through backtracking. This means that some assignments need to be taken back. Looking at this from the other direction, it can very well happen, that the same variable can occur in decisions multiple times over such a calculation. If a variable assignment is even slightly involved in a conflict, then, lacking any further knowledge, it is a good strategy to simply try it's negation in the future. It is not guaranteed, but simply more likely that the other assignment resolves the conflict that occurred with the previous assignment. 

The default way in that preferences are implemented stands in the way of this. If you tell the SAT solver to always assign the same boolean value to a specific variable, then the strategy above cannot be applied. That is why I have implemented an option to $PB1$ where preferences are forgotten after the first time when they are taken into account. This means that when a variable is selected for a decision, it is removed from the primary heap that is consulted first for this selection\footnote{
	In the Sat4J library, what decides the order of decisions and what decides the value assigned in these decisions are actually two separate data structures. This means that this removal must be written in two separate places.
}. 

The secondary heap always contains all used variables for selection, but in the default configuration where preferences are not forgotten the preferred variables would already be assigned at the point in time when it is consulted. Here this means, that once a preference was forgotten, the associated variables can still occur in decisions after they were reverted through backtracking and can potentially be assigned a different value. 

The most important effect of this strategy is, that the solver quickly falls back to his default behaviour when it turns out that the formula is difficult to solve when preferences for the resulting model are to be taken into account. 

It is quite likely that a formula contains models that are relevant for the backbone, because they contain the only occurence of some literal. However, if they contradict the current set of preferred literals in many other places, giving these preferences to the SAT solver can actually make it difficult to find said model.

\section{Subsets}
During benchmarking it showed itself, that all solvers that did not employ any preferences at all were the fastest ones when it came to difficult formulas. So it might be interesting to test some algorithm that can be configured with a floating point number, where a ground value of $0.0$ would be equivalent to having no preferences at all and then allowing to enable preferences in very small increments.

The simplest way to do this would be to restrict the number of preferences that could be set for each SAT call. The parameter above is multiplied with the number of variables in the formula and then the number of preferred literals is pruned in each iteration to have at most that size, without of course discarding the set of assignments that has yet to be tested.

Aside from the straightforward implementation, I also created a variant, where extra care is taken to use different subsets for the preferences for each sat call. To do this, all preferences given to the SAT solver are remembered and not picked again until all of them were tried once. At this point the memory of previously used preferences is cleared so that it builds up again.


%Some preferences are difficult to take into account, but it is still possible that they will eventually disappear through model reduction, without spending too much time on finding a model with them. TODO irgendwo unterbringen


%\section{Inverted Preferences}
%dropped the topic of inverted preferences

\section{Nudging}
The previous approach to scalable preferences still works against the decision heuristic because before the SAT solver even starts, the preferred variables are chosen. What subset of the preferred variables would be easiest for the solver to take into account cannot be said upfront. Therefore it should be much more efficient to "nudge" the existing decision heuristic in the direction of the preferences while it's running.

To do this, I implemented a different kind of preference strategy. Here, similar to the previous method, you can pass a floating point number $f$ between $0.0$ and $1.0$ in addition to the set of preferred variables. When a decision happens in the CDCL algorithm, the selection heuristic sorts all variables by an activity value, which is typically based on how often it was involved in conflicts. Then the resulting array is scanned, beginning with the variable with the lowest activity until one is found that is not yet assigned. 

Here this activity value is multiplied with $1-f$ during this sorting step where a variable is preferred. Therefore, if you pass $0$ as value for $f$, the order that is returned from sorting the activities would be completely unaffected. Very small values of $f$ would only affect the selection heuristic if a preferred value was already deemed very important but only in second or third place with a small distance to the first one. 

However, this factor $f$ can only influence the order of variables for decisions. Which value is then assigned can not be weighted\footnote{Except if there was a heuristic for that, which can give a confidence value}. That is why the default strategy for deciding the phase was left in place. 

% könnte besser sein dass präferierte literale entschieden werden. Unit assignments können nicht optional in model reduction sein... FALSCH gilt nur für primimplikanten reduktion

The purpose of this scheme was to see, whether benefits of preferences would rise up faster than the drawbacks with very low values of $f$.

\section{Approaching lower and upper bounds}
I also tried an idea that was shortly mentioned in \cite[Chapter 3.4]{mjl10}: \blockquote{Another approach consists of executing enumeration and iteration based algorithms in parallel, since enumeration refines upper bounds on the size of the backbone, and iteration refines lower bounds. Such algorithm could terminate as soon as both bounds become equal.}

TODO fertig machen pb1e geht so, vonThore funktioniert damit super.
...
In fact, you could say that the $BB$ algorithm matches this concept, since it reduces the set of literals to further examine with every found model.

