\section{Second industrial benchmark}
\begin{wraptable}{r}{7cm}
\begin{tabular}{l| c c c c}
& $n_{Literals}$ & $n_{Clauses}$ & $n_{test}$ & Filesize \\
\hline
$POF_1$ & 469 & 66957 & 407 & 817 KiB \\
$POF_2$ & 5055 & 81267 & 948 & 1401 KiB \\
\end{tabular}
\caption{Size comparison of the two industrial benchmarks. Contains the number of literals, clauses, tested literals and the filesize. }
\label{tab:indusComp}
\end{wraptable}

In the following I will analyze another product formula $POF_2$, similar the one in the previous chapter. However this one is much larger than the previous, see table \ref{tab:indusComp}. I tested this formula against the same set of algorithms as the previous formula $POF_1$ including $PB4$, to identify the specific characteristics of it and identify the optimal backbone solver for it, just as in the previous section. Table \ref{tab:vonThore2pof} shows the performance result. 

\begin{wraptable}{R}{8cm}
\begin{tabular}{l| c c c c}
& $t_{calc}$ & $t_{sat}$ & $t_{last}$ & $n_{sat}$ \\
 \hline
BB & 1007.665 & 712.676 & - & 2063982 \\
IBB & 1291.489 & 866.561 & - & 432408 \\
KBB & 1253.658 & 445.423 & - & 417846 \\
PB0 & 442.172 & 285.674 & 2.139 & 129943 \\
PB1 & 523.182 & 331.089 & 0.211 & 146349 \\
PB1(amnes) & 504.572 & 315.742 & 0.229 & 143503 \\
PB1(model) & 391.147 & 340.538 & 0.252 & 151119 \\
%PB1x & 1420.34 & 910.275 & 1.675 & 440355 \\
%PB1a & 948.42 & 646.993 & 2.576 & 239787 \\
PB1b & 1421.294 & 869.288 & 0.617 & 460292 \\
PB1c(50\%) & 533.995 & 334.392 & 0.232 & 146799 \\
PB1c(5\%) & 614.968 & 377.302 & 3.648 & 198227 \\
PB1d & 542.427 & 332.594 & 0.217 & 146430 \\
PB1e & 523.955 & 324.705 & 1.164 & 148096 \\
%PB1f & 522.015 & 327.188 & 1.244 & 147614 \\
PB2(50\%) & 1660.442 & 1123.058 & 4.227 & 438298 \\
PB2(5\%) & 1591.691 & 1052.998 & 2.039 & 439801 \\
PB2(0.5\%) & 1586.361 & 1049.304 & 2 & 439140 \\
PB3 & 572.575 & 325.808 & 0.265 & 141076 \\
PB4 & 2478.283 & 2361.488 & 0.2 &297,192 \\
\end{tabular}
\caption{Second Industrial benchmark. Values are not averaged, but summed up over 948 different benchmarks.}
\label{tab:vonThore2pof} %for referencing
\end{wraptable}

The first thing that shows, again, is that algorithms with preferences have a much better performance overall. $PB0$ and $PB1$ finish between two and three times faster than $BB$ and $IBB$

PB0+BB sind gut -> hinweis auf gelernte klauseln

%The first thing that shows is that $PB4$, the algorithm proposed in the previous section performs quite bad. . We will have to find something new if the plan is to solve this particular formula as efficiently as possible. \footnote{Otherwise, $PB1(model)$ seems to be a good candidate, maybe also $PB0$ without model reduction}

analyse

PB4 bringt nix\\
präferenzen immer besser\\
gelernte literale bringen nichts.\\
rotieren < primimplikante < model bzgl calcTime, in pureSat andersrum, aber logisch, weil stärkere reduktion. model reduktion hier anscheinend völlig nutzlos und verschwendet zeit\\
PB0 besser als PB1. \\
PB0 komischerweise schlechtere last sat time TODO entweder begründen oder einfach weglassen\\
PB1(amnes) geht auch gut\\
aber: basisformel hat backbones und lernbare klauseln\\

Further minor observations findings:
\begin{itemize}
	\item The observation about unit implied backbone literals can be observed again in this benchmark. The number of SAT calls and the time spent in the SAT solver are much better in $KBB$ compared to $BB$. But this does not reflect in better overall performance.
	\item The algorithm designed for the previous benchmark, $PB4$ performs quite bad. It is obviously optimized for that particular formula and POFs do not make a class of formulas where you find all of the characteristics that were identified in $POF_1$.
\end{itemize}
PB0 geht mit (model), leider nicht auch mit (amnes)
Basis formel hat backbone literale, also de




ad

ad

ad

ad

ad

ad

ad

ad

ad

ad

ad

ad

ad

ad

ad

wichtig: die literale nicht per klausel erzwingen sondern per assumption, dann werden gelernte klauseln nicht weggeworfen. geht nur mit BB und PB0, und bringt nix in vonThore, weil da nix gelernt wird.

\begin{wraptable}{l}{8cm}
\begin{tabular}{l| c c c c}
& $t_{calc}$ & $t_{sat}$ & $t_{last}$ & $n_{sat}$ \\
\hline
BB & 875.153 & 565.083 & - & 2,063,224\\
PB0 & 407.427 & 197.473 & 1.54 & 129721\\
PB0(model) & 229.986 & 197.334 & 1.48 & 129742\\
\end{tabular}
\caption{Results with assumptions instead of formula modification}
\label{tab:pofAssump}
\end{wraptable}




\subsection{Efficiency over all assumptions}
Since for this benchmark, the almost same formula is worked on hundreds of times, it makes sense to think about ways how to make this outer loop efficient as well.

eigentlich erstes: kopieren rausoptimieren

The first consideration was to first compute information of the base formula and reuse it in all subsequent computations, where the backbone under an assumption was calculated. Two interesting pieces of this would be the clauses that would be learned by doing this and secondly the backbone of the base formula.

The gain of this depends on the formula. $POF_1$ by itself actually had neither backbone literals nor was it necessary for the SAT solver to learn any new clauses to find a model for it, and even when I tried to compute it's backbone with either $PB0$ or $BB$\footnote{Both algorithms, that can leave their learned clauses in place after backbone computation.} in an effort to learn as much about the formula as possible, not a single clause was learned.

$POF_2$ on the other hand actually had quite a large backbone and many clauses to learn. Table \ref{tab:pofPrepBenefit} shows performance results where the solver object was always copied from either the original formula or one where learned clauses and backbone literals were added explicitely. The third ssection shows how long the preparations took with different solvers and how many clauses were learned and additionally the size of $POF_2$'s backbone.

\begin{wraptable}{r}{8cm}
\begin{tabular}{l | c c c c}
$Computation$ & $t_{full}$ & $t_{sat}$ & $t_{last}$& $n_{sat}$\\
\hline 
\multicolumn{3}{l}{Without preperation}\\
BB & 905.379 & 626.052 & - & 2063982\\
PB0 & 413.299 & 261.777 & 1.985 & 129943\\
PB0(model) & 293.795 & 259.836 & 1.957 & 130099\\
PB1 & 489.417 & 305.337 & 0.191 & 146349\\
PB1(model) & 366.219 & 314.409 & 0.2 & 151119\\
%PB4 & 2274.757 & 2158.624 & 9.609 & 297192\\
\hline
\multicolumn{5}{l}{With backbones and learned clauses from $PB0$}\\
BB & 894.491 & 615.687 & - & 2064493\\
PB0 & 393.149 & 243.261 & 2.004 & 129908\\
PB0(model) & 275.352 & 241.019 & 2 & 129854\\
PB1 & 459.353 & 279.284 & 0.217 & 144057\\
PB1(model) & 341.306 & 290.019 & 0.244 & 148868\\
%PB4 & 2093.709 & 1941.915 & 3.343 & 284062\\
\hline \hline 
$Preparation$ & $t_{calc}$ & $n_{sat}$& $n_{learned}$ & $n_{BB}$  \\
\hline
BB         & 1.122 &1883& 1898 & 1449\\
PB0        & 0.703 &217& 2214 & 1449\\
PB0(model) & 0.494 &214& 2244 & 1449 \\
TODO &nochmal& ausrechnen& lassen
\end{tabular}
\caption{Comparison of benchmark results depending on reuse of learned clauses and backbones of the base formula $POF_2$. Subsequently number of learned clauses and backbone literals through preparation. TODO irgendwie erklären wieso konsistent besser als in basistabelle}
\label{tab:pofPrepBenefit}
\end{wraptable}

learned lits übernehmen in POF ist schlechter ansatz, weil wird relativ schnell schlechter als immer die formel übernehmen, womöglich nutzlose learned clauses, die nur verlangsamen und nichts aussagen (weil von anderen assumptions)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The first consideration was to first compute information of the base formula and reuse it in all subsequent computations, where the backbone under an assumption was calculated. However this was not possible.\\
One interesting piece of information would have been the set of clauses that were learned during the computation of a model of the base formula $POF_1$. However this set was empty. Appearantly the formula is simple enough, that the model can be found without a single conflict. The solver returned without having learned any new clause for the formula.

Another thing I thought about was the backbone of the base formula. If you can determine backbone literals of $POF_1$, then you can safely assume, that all of these are part of any restricted formula $POF_1 \cup \{a\}$. This is because, when a clause gets added to a CNF formula, it's set of models can only shrink, no new model can be induced through an additional clause\footnote{Assuming that no new variable was introduced.}. This implies that an assignment that didn't occur in the models before will also not appear in the new smaller set of models, in turn implying that if a variable always occured with the same assignment in $POF_1$, it will also do so in $POF_1 \cup \{a\}$. \\
However this set was empty as well and for a good reason in this particular case. The backbone of a formula consists of all assignments that have to be done to end up in a model. In the context of a product formula, which we have here, this would mean, that a specific feature of the product would always have to be selected. In other words, the product has an option that is not optional. 

What remained was to see, if the loop over the assumptions could be sped up. In fact whereas the backbone computations\footnote{Of only the new $PB4$ solver} took a sum of 1.5 seconds, this loop overall took 3.9 seconds. The difference consisted of primarily the copying of the formula(2.2 seconds) and writing down benchmark information(around 150 milliseconds). 

To do this in $Sat4J$ you must first of all reset the formula object\footnote{The class name in $Sat4J$ is $Solver$ or $ISolver$.}. In the case of model enumeration algorithms like $IBB$ or $PB1$, this incorporates removing all the blocking clauses because they modify the formula. If you don't do this, all backbone computations after the first one will immediately fail, because the final blocking clause makes the formula unsatisfiable. Additionally, you must flush the set of learned clauses and literals because some of these may be based on the blocking clauses. Therefore, if you would leave them in place, they could reproduce some of the effects that the blocking clause had. Take care that all of this happens in a way that during the individual backbone computations, the learned clauses stay in place. $Sat4J$ supplies a special function to remove a clause under the expectation that it will immediately become subsumed or already is, which you can use to remove old blocking clauses during the loop without flushing the learned clauses. Alternatively you could remove all of them at the end. The ordinary function to remove a clause in $Sat4J$ triggers the cleanup automatically. 

Additionally, it makes sense to revert the decision heuristic if you replaced it with something that allows to implement preferences and you want to reuse the formula object with a backbone solver without preferences. The typical behaviour throughout the $Sat4J$ library, including the $BB$ and $IBB$ backbone solvers, is to assume that such a data structure is already set up correctly.

One last bit of state inside the formula object that I noticed were two floating point numbers called $claInc$ and $claDecay$. These influence the scale in which the activity values inside the decision heuristic grow and shrink. They do not influence the calculation result, but merely the exact behaviour of the solver.

With all of this taken care of, the duration of the outermost loop over the assumptions took just as long as the sum of the durations of the backbone computations.