\section{SAT Competition Benchmark}

For the first benchmark, I collected a set of 64 files from a SAT competition TODO welcher. The SAT competitions generally use problems that are difficult to solve compared to other problems of the same file size. This is in order to encourage development of solving strategies that reliably have good performance and not only for most of them. To save time during benchmarking, those files that took longer than around one minute for a single model computation were excluded, resulting in files that are generally around 30 kilobytes large. Additionally, problems where the duration for backbone computation averaged below one second were excluded, because here the small differences in the measurements could just as well be explained with external factors like the CPU throttling for a short time. To get meaningful testresults for such files, multiple testpasses should be conducted.

\begin{wraptable}{r}{7cm} %[tbp]
\begin{tabular}{l| c c c }
 & $t_{full}$ & $t_{sat}$ & $n_{sat}$ \\
 \hline
BB & 8.63 & 8.628 & 254 \\
IBB & 4.946 & 4.944 & 9 \\
KBB & 8.713 & 8.687 & 36 \\
PB0 & 31.78 & 31.779 & 4 \\
PB1 & 17.49 & 17.488 & 5 \\
PB1(amnes) & 10.867 & 10.865 & 6 \\
PB1(model) & 25.794 & 25.793 & 5 \\
PB1x & 5.064 & 5.062 & 9 \\
PB1a & 11.241 & 11.239 & 8 \\
PB1b & 9.785 & 9.783 & 6 \\
PB1c & 25.312 & 25.311 & 5 \\
PB1c(5\%) & 1064.097 & 1064.095 & 6 \\
PB1d & 26.746 & 26.745 & 5 \\
PB1e & 17.187 & 16.591 & 6 \\
PB1f & 15.971 & 15.97 & 6 \\
PB2 & 10.135 & 10.133 & 10 \\
PB2(5\%) & 5.232 & 5.229 & 9 \\
PB2(0.5\%) & 5.251 & 5.249 & 9 \\
PB3 & 20.927 & 20.922 & 4 \\
\end{tabular}
\caption{Averages of 64 testfiles taken from sat competitions. The columns indicate: The full time that the calculation took in seconds; The time that was spent in the sat solver; The number of sat calls (all three values are averages). }
\label{tab:satCompAvg} %for referencing
\end{wraptable}

The averaged time to compute the backbones of these 64 testfiles can be seen in \ref{tab:satCompAvg}. To prove that various computation strategies are implemented as good as possible\footnote{e.g. no noteworthy timeloss during setup of preferences}, the second column shows the time that was only spent in the sat solver.

Taking a look at this table, we can quickly see that for these instances, all solvers that employed preferences as part of their algorithm performed much worse than those that did not\footnote{Which would be $BB$,$IBB$,$KBB$ and $PB1x$}.

%You can also see a vague inverse correlation between the number of sat calls and the calculation time. wenige sat calls heissen nicht automatische wenig rechenzeit, womöglich aber auch nur weil hier eine schwierige datei ist.

%TODO IBB hatte bug, darum bessere leistung von pb1x vgl IBB\\
%zweite tabelle zeigen, wenn blocking clause nicht entfernt wurde

\subsection{Importance of reusing learned clauses}
Table \ref{tab:learnedIbb} shows a comparison of individual benchmarks to highlight the importance of reusing learned clauses. While working with the $Sat4J$ library I noticed that the $IBB$ backbone algorithm was accidentally configured in a way that learned clauses would always be discarded between SAT computations. However, these learned clauses are still valid in later iterations of the $IBB$ algorithm. The only difference that the formula goes through during this algorithm, is that the blocking clause that ensures a new model repeatedly looses some of it's literals. As long as the set of solutions for a formula is only reduced, the learned clauses of that formula stay valid.\footnote{Example: With a formula without any clauses but three variablesyy $a$,$b$ and $c$ you can create eight models. With only the clause $\{a\}$ in your formula you can have four models, with only the clause $\{a \lor b\}$ you can have 6 models, with only the clause $\{a\lor b \lor c\}$ you can have 7 models.}

\begin{wraptable}{R}{8cm}
\begin{tabular}{l| c c }
File & $t_{keep}$ & $t_{discard}$ \\
\hline
brock400-2.cnf & 0.233 & 0.252 \\
dimacs-hanoi5.cnf & 1.41 & 1.596 \\
grieu-vmpc-s05-25.cnf & 71.945 & 78.964 \\
grieu-vmpc-s05-27.cnf & 554.52 & 648.697 \\
%johnson8-2-4.cnf & 0.001 & 0.001 \\
fla-barthel-200-2.cnf & 0.634 & 6.019 \\
fla-barthel-200-3.cnf & 0.619 & 2.16 \\
fla-barthel-220-1.cnf & 2.511 & 9.572 \\
fla-barthel-220-2.cnf & 7.497 & 17.759 \\
fla-barthel-220-4.cnf & 2.24 & 14.113 \\
fla-barthel-240-2.cnf & 3.632 & 50.552 \\
fla-komb-200-3.cnf & 0.236 & 0.301 \\
fla-komb-200-5.cnf & 0.114 & 0.276 \\
\iffalse
fla-komb-220-1.cnf & 0.293 & 0.475 \\
fla-komb-220-3.cnf & 0.229 & 0.466 \\
fla-komb-220-4.cnf & 0.129 & 0.217 \\
fla-komb-220-5.cnf & 0.167 & 0.247 \\
fla-komb-240-2.cnf & 0.326 & 0.413 \\
fla-komb-240-3.cnf & 0.395 & 0.392 \\
fla-komb-240-5.cnf & 0.478 & 0.481 \\
fla-komb-260-1.cnf & 1.518 & 3.032 \\
fla-komb-260-3.cnf & 1.964 & 3.093 \\
fla-komb-260-4.cnf & 1.452 & 1.469 \\
fla-komb-280-1.cnf & 3.625 & 4.384 \\
fla-komb-280-3.cnf & 0.926 & 2.375 \\
fla-komb-280-4.cnf & 1.438 & 1.442 \\
fla-komb-280-5.cnf & 2.678 & 4.124 \\
fla-komb-300-3.cnf & 3.46 & 8.989 \\
fla-komb-300-4.cnf & 2.72 & 5.816 \\
fla-komb-300-5.cnf & 2.095 & 2.092 \\
fla-komb-320-2.cnf & 2.649 & 4.34 \\
fla-komb-320-3.cnf & 10.821 & 10.135 \\
fla-komb-320-5.cnf & 8.367 & 15.784 \\
fla-komb-340-1.cnf & 4.453 & 7.647 \\
fla-komb-340-2.cnf & 10.553 & 18.844 \\
fla-komb-340-3.cnf & 9.665 & 9.619 \\
fla-komb-340-4.cnf & 19.507 & 30.767 \\
fla-komb-340-5.cnf & 11.085 & 16.571 \\
fla-komb-360-1.cnf & 9.639 & 11.066 \\
fla-komb-360-4.cnf & 13.362 & 22.839 \\
fla-komb-360-5.cnf & 19.295 & 35.347 \\
fla-komb-380-2.cnf & 41.745 & 89.802 \\
fla-komb-380-3.cnf & 43.062 & 59.364 \\
fla-komb-380-4.cnf & 105.37 & 179.518 \\
fla-komb-380-5.cnf & 57.441 & 107.179 \\
fla-qhid-200-1.cnf & 0.063 & 0.063 \\
fla-qhid-200-2.cnf & 0.127 & 0.122 \\
fla-qhid-200-4.cnf & 0.198 & 0.201 \\
fla-qhid-200-5.cnf & 0.162 & 0.163 \\
fla-qhid-220-2.cnf & 0.356 & 0.525 \\
fla-qhid-220-3.cnf & 0.231 & 0.355 \\
fla-qhid-220-4.cnf & 0.34 & 0.412 \\
fla-qhid-220-5.cnf & 0.399 & 0.504 \\
fla-qhid-240-3.cnf & 1.035 & 3.079 \\
fla-qhid-240-5.cnf & 0.624 & 0.724 \\
fla-qhid-260-1.cnf & 0.838 & 2.389 \\
fla-qhid-260-2.cnf & 0.602 & 0.613 \\
fla-qhid-280-1.cnf & 2.948 & 18.6 \\
fla-qhid-280-3.cnf & 1.592 & 2.516 \\
fla-qhid-300-1.cnf & 2.103 & 2.108 \\
fla-qhid-300-4.cnf & 3.385 & 3.741 \\
\fi
fla-qhid-320-1.cnf & 6.61 & 6.575 \\
fla-qhid-320-2.cnf & 9.227 & 101.17 \\
fla-qhid-320-5.cnf & 7.861 & 7.962 \\
fla-qhid-340-2.cnf & 11.922 & 13.688 \\
fla-qhid-340-3.cnf & 11.796 & 17.157 \\
fla-qhid-340-4.cnf & 9.454 & 9.297 \\
fla-qhid-360-1.cnf & 39.998 & 39.797 \\
fla-qhid-360-5.cnf & 10.698 & 10.849 \\
fla-qhid-380-1.cnf & 189.895 & 249.003 \\
fla-qhid-400-3.cnf & 83.098 & 130.834 \\
fla-qhid-400-4.cnf & 55.213 & 52.811 \\
smallSatBomb.cnf & 0.011 & 0.022 \\
\end{tabular}
\caption{Backbone computation time of the $IBB$ algorithm, once with keeping learned clauses ($t_{keep}$) and once discarding learned clauses between every sat call($t_{discard}$)}
\label{tab:learnedIbb} %for referencing
\end{wraptable}

The information contained in learned clauses is very valuable, as it prevents the solver from repeating invalid combinations of assignments that might even be likely to occur again. But if already learned clauses can guide the SAT solver away from possible conflicts, it could ideally return a model without any backtracking.

\iffalse
\subsection{Comparison of PB0 with PB1}

\begin{wraptable}[7]{r}{7cm} %[tbp]
\begin{tabular}{l| c c c c}
& $t_{full}$ & $t_{sat}$ & $t_{last}$ & $n_{sat}$ \\
\hline			
PB0 & 1005.756 & 1005.753 & 810.946 & 3 \\
PB1 & 6741.638 & 6741.635 & 6594.098 & 2 \\
IBB & 556.815 & 556.719 & 361.825 & 2 \\
\end{tabular}
\caption{Comparison of runtime of file $grieu-vmpc-s05-27$ . $t_{last}$ shows the time that the very last sat call took.}
\label{tab:tLastGrieu} %for referencing
\end{wraptable}
Table \ref{tab:satCompAvg} shows a strong performance difference between PB0 and PB1.

 grieu beispiel rauspicken (letzter sat call)


stammt daher, dass zuletzt unsat errechnet wird. vonThore hat das problem nicht (selten überhaupt eine millisekunde gebraucht für letzten aufruf)
\fi


\subsection{Benefits of unit implication}
\label{ss:result_unit}
This section evaluates the effects of trying to recognize backbone literals through unit implication as described in section \ref{subsec:unitImpl}. Table \ref{tab:satCompAvg} interestingly shows no performance benefit for this technique, even though the number of sat calls is much smaller.



\begin{table} %[tbp]
\begin{tabular}{l| c c c c }
File & $t_{BB}[n_{sat}]$ & $t_{KBB}(t_{sat})[n_{sat}]$ & $n_{unit}$ & $n_{backbone}$  \\
\hline
brock400-2.cnf & 0.04[254] & 0.04(0.03)[254] & 0 & 0 \\
fla-komb-400-3.cnf & 1276.71[381] & 1241.12(1241.11)[45] & 336 & 379 \\
dimacs-hanoi5.cnf & 1.94[1931] & 1.84(1.23)[281] & 1650 & 1931 \\
vonThore42.cnf & 0.02[398] & 0.01(0.01)[51] & 345 & 346 \\
grieu-vmpc-s05-27.cnf & 261.09[678] & 281.08(277.1)[536] & 142 & 677 \\
fla-komb-360-4.cnf & 14.94[337] & 14.94(14.9)[45] & 292 & 333 \\
fla-qhid-360-1.cnf & 74.96[355] & 76.03(76.03)[29] & 326 & 355 \\
grieu-vmpc-s05-25.cnf & 182.84[625] & 179.91(179.44)[51] & 574 & 625 \\
fla-qhid-360-5.cnf & 20.68[350] & 20.89(20.89)[29] & 321 & 349 \\
fla-barthel-220-4.cnf & 3.55[32] & 2.74(2.73)[32] & 0 & 6 \\
9012345.cnf & 4.18[1483] & 6.73(5.92)[813] & 670 & 1478 \\
fla-barthel-220-2.cnf & 8.18[23] & 8.11(8.11)[23] & 0 & 4 \\
1098765.cnf & 1.21[938] & 1.29(0.83)[493] & 444 & 921 \\
smallSatBomb.cnf & 0.01[26] & 0.01(0.01)[19] & 7 & 9 \\
\end{tabular}
\caption{Benchmark results for a selection of files with a focus on the benefit of unit implication.
Rows indicate: Calculation time and number of SAT calls for the $BB$ solver  ; Calculation time, time spent in SAT solver and number of SAT calls for the $KBB$ solver ; Number of backbone literals identified through unit implication (in $KBB$) ; Number of backbone variables in formula.}
\label{tab:bbkbb}
\end{table}

Table \ref{tab:bbkbb}\footnote{
	In this table the pure sat calculation time for the $BB$ column is missing. The $BB$ algorithm actually spends almost no time outside of the SAT solver in the case of the listed formulas.\\
	Most of the performance differences in this table can be explained with other reasons than actual algorithmic differences. $fla-barthel-220-4.cnf$ for example should not be faster with $KBB$ since here no backbone literal was determined through unit propagation. However I was able to reproduce this performance difference just through the order of what was called first, i.e. if the two solvers are called the other way around the difference in timing between the first and second call are the same. A possible explanation could be a power saving feature in modern processors.
	} 
shows individual performance differences between the $BB$ solver supplied by Sat4J and my own $KBB$ solver for comparison, as well as the backbone's size and the number of backbone literals identified through unit implication. The time columns also contain the number of sat calls that were comitted. It's content matches the results of table \ref{tab:satCompAvg}. Occasional cases where $KBB$ trumps in performance over $BB$ get balanced out by cases where this is the other way round, but the number of sat calls is always better with the $KBB$ algorithm. And this is true in spite of the fact that almost all identifications happened through unit implication. 

This indicates that the effect from scanning for unit implied backbone literals also appears in the $BB$ solver, only not quite as obvious. When you look at line \ref{algo:bb:learn} of algorithm \ref{alg:bb}, you see that $BB$ actually takes identified backbone literals up into the formula to speed up the solving process. Now imagine what happens, if the unit implication case happens during the course of the $BB$ algorithm. You have a clause that is not satisfied, even though all but one of it's literals are already assigned through the learned backbone literals. When $BB$ tests the last of that clauses literals with an assumption against the last one, the clause becomes immediately unsatisfied\footnote{Remember that $BB$ tests by trying to disprove a potential backbone literal.}, proving the last remaining literal in the clause is part of the backbone. 

The results of the next chapter paint a slightly different picture. Here the pure SAT time of $KBB$ is actually smaller than that of $BB$, however even $BB$ looses around a third of it's calculation time outside of pure SAT calls. This could be explained by the much higher number of SAT calls for this benchmark and the overhead that comes with preparations and cleanups for a SAT call, which appearantly became relevant at that point. Given good conditions (like in that benchmark) $KBB$ can identify many backbone literals in a single search, which also requires less data structures around it compared to a complete SAT computation.

\subsection{Benefits of forgetting preferences}
schlechte ergebnisse von PB1c (0.05) vgl run2 mit run3 (run3 hat kein default vergessen)
