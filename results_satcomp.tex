\section{SAT competition benchmark}
\label{sec:satcompBench}
\begin{wraptable}{r}{8.5cm} %[tbp]
\begin{tabular}{l| c c c c}
 & $t_{full}$ & $t_{sat}$ & $t_{last}$& $n_{sat}$ \\
 \hline
$BB$ & 8.63 & 8.628 & - & 254\\
$BB_{unit}$ & 8.713 & 8.687 & - & 36\\
$IBB$ & 4.946 & 4.944 & 1.911 & 9\\
$PB0$ & 31.78 & 31.779 & 1.17 & 4\\
$PB1$ & 17.49 & 17.488 & 6.998 & 5\\
$PB1_{model}$ & 25.794 & 25.793 & 8.677 & 5\\
$PB1_{rotate}$ & 20.927 & 20.922 & 8.912 & 4\\
$PB1_{forget}$ & 10.867 & 10.865 & 4.781 & 6\\
%PB1x & 5.064 & 5.062 & 1.562 & 9\\
%PB1a & 11.241 & 11.239 & 5.351 & 8\\
$PB1b$ & 9.785 & 9.783 & 1.921 & 6\\
$PB1c_{50\%}$ & 25.312 & 25.311 & 12.909 & 5\\
$PB1c_{5\%}$ & 1064.097 & 1064.095 & 797.244 & 6\\
$PB1d_{50\%}$ & 26.746 & 26.745 & 11.284 & 5\\
$PB1e$ & 17.187 & 16.591 & 6.796 & 6\\
%PB1f & 15.971 & 15.97 & 4.333 & 6\\
$PB2_{50\%}$ & 10.135 & 10.133 & 6.428 & 10\\
$PB2_{5\%}$ & 5.232 & 5.229 & 1.737 & 9\\
$PB2_{0.5\%}$ & 5.251 & 5.249 & 1.816 & 9\\
\end{tabular}
\caption[Performance results of SAT competition benchmark]{Averaged performance results of 64 testfiles taken from SAT competitions. The columns indicate: The full time that the calculation took in seconds; The time that was spent in the SAT solver; The time that the last SAT computation took; The number of SAT calls (all values are averages).}
\label{tab:satCompAvg} %for referencing
\end{wraptable}

For the first benchmark, I collected a set of 64 files from the 2017 SAT competition\footnote{
To be more precise the $essential$ folder from the $incremental$ package, currently available at \url{https://baldur.iti.kit.edu/sat-competition-2017/benchmarks/incremental.zip}
}. The SAT competitions generally use problems that are difficult to solve compared to other problems of the same file size. This is in order to encourage development of solving strategies that reliably have good performance and not only for most of them. To save time during benchmarking, files that took longer than around one minute for a single model computation were excluded, resulting in files that are generally around 30 kilobytes large. Additionally, problems where the duration for backbone computation averaged below one second were excluded, because here the small differences in the measurements could just as well be explained with external factors like short variations in the clock speed of the CPU. To get meaningful testresults for such files, multiple testpasses should be conducted.

The averaged time to compute the backbones of these 64 testfiles can be seen in \ref{tab:satCompAvg}\footnote{
	The hardware used for all benchmarks listed in this thesis was an Intel Core i7-8809G with 16 gigabytes of DDR4-2667 memory in dual channel and an NVMe harddrive. 
}. The second column shows the time that was only spent in the SAT solver. This gives a hint on whether a particular algorithm configures its SAT solver well and whether it looses computation time in things besides the SAT calls, for example through model reduction.

\begin{wraptable}[29]{R}{7.5cm}
\begin{tabular}{l| c c }
File & $t_{keep}$ & $t_{discard}$ \\
\hline
brock400-2.cnf & 0.233 & 0.252 \\
dimacs-hanoi5.cnf & 1.41 & 1.596 \\
grieu-vmpc-s05-25.cnf & 71.945 & 78.964 \\
grieu-vmpc-s05-27.cnf & 554.52 & 648.697 \\
%johnson8-2-4.cnf & 0.001 & 0.001 \\
fla-barthel-200-2.cnf & 0.634 & 6.019 \\
fla-barthel-200-3.cnf & 0.619 & 2.16 \\
fla-barthel-220-1.cnf & 2.511 & 9.572 \\
fla-barthel-220-2.cnf & 7.497 & 17.759 \\
fla-barthel-220-4.cnf & 2.24 & 14.113 \\
fla-barthel-240-2.cnf & 3.632 & 50.552 \\
\iffalse
fla-komb-200-3.cnf & 0.236 & 0.301 \\
fla-komb-200-5.cnf & 0.114 & 0.276 \\
fla-komb-220-1.cnf & 0.293 & 0.475 \\
fla-komb-220-3.cnf & 0.229 & 0.466 \\
fla-komb-220-4.cnf & 0.129 & 0.217 \\
fla-komb-220-5.cnf & 0.167 & 0.247 \\
fla-komb-240-2.cnf & 0.326 & 0.413 \\
fla-komb-240-3.cnf & 0.395 & 0.392 \\
fla-komb-240-5.cnf & 0.478 & 0.481 \\
fla-komb-260-1.cnf & 1.518 & 3.032 \\
fla-komb-260-3.cnf & 1.964 & 3.093 \\
fla-komb-260-4.cnf & 1.452 & 1.469 \\
fla-komb-280-1.cnf & 3.625 & 4.384 \\
fla-komb-280-3.cnf & 0.926 & 2.375 \\
fla-komb-280-4.cnf & 1.438 & 1.442 \\
fla-komb-280-5.cnf & 2.678 & 4.124 \\
fla-komb-300-3.cnf & 3.46 & 8.989 \\
fla-komb-300-4.cnf & 2.72 & 5.816 \\
fla-komb-300-5.cnf & 2.095 & 2.092 \\
fla-komb-320-2.cnf & 2.649 & 4.34 \\
fla-komb-320-3.cnf & 10.821 & 10.135 \\
fla-komb-320-5.cnf & 8.367 & 15.784 \\
fla-komb-340-1.cnf & 4.453 & 7.647 \\
fla-komb-340-2.cnf & 10.553 & 18.844 \\
fla-komb-340-3.cnf & 9.665 & 9.619 \\
fla-komb-340-4.cnf & 19.507 & 30.767 \\
fla-komb-340-5.cnf & 11.085 & 16.571 \\
fla-komb-360-1.cnf & 9.639 & 11.066 \\
fla-komb-360-4.cnf & 13.362 & 22.839 \\
fla-komb-360-5.cnf & 19.295 & 35.347 \\
fla-komb-380-2.cnf & 41.745 & 89.802 \\
fla-komb-380-3.cnf & 43.062 & 59.364 \\
fla-komb-380-4.cnf & 105.37 & 179.518 \\
fla-komb-380-5.cnf & 57.441 & 107.179 \\
fla-qhid-200-1.cnf & 0.063 & 0.063 \\
fla-qhid-200-2.cnf & 0.127 & 0.122 \\
fla-qhid-200-4.cnf & 0.198 & 0.201 \\
fla-qhid-200-5.cnf & 0.162 & 0.163 \\
fla-qhid-220-2.cnf & 0.356 & 0.525 \\
fla-qhid-220-3.cnf & 0.231 & 0.355 \\
fla-qhid-220-4.cnf & 0.34 & 0.412 \\
fla-qhid-220-5.cnf & 0.399 & 0.504 \\
fla-qhid-240-3.cnf & 1.035 & 3.079 \\
fla-qhid-240-5.cnf & 0.624 & 0.724 \\
fla-qhid-260-1.cnf & 0.838 & 2.389 \\
fla-qhid-260-2.cnf & 0.602 & 0.613 \\
fla-qhid-280-1.cnf & 2.948 & 18.6 \\
fla-qhid-280-3.cnf & 1.592 & 2.516 \\
fla-qhid-300-1.cnf & 2.103 & 2.108 \\
fla-qhid-300-4.cnf & 3.385 & 3.741 \\
\fi
fla-qhid-320-1.cnf & 6.61 & 6.575 \\
fla-qhid-320-2.cnf & 9.227 & 101.17 \\
fla-qhid-320-5.cnf & 7.861 & 7.962 \\
fla-qhid-340-2.cnf & 11.922 & 13.688 \\
fla-qhid-340-3.cnf & 11.796 & 17.157 \\
fla-qhid-340-4.cnf & 9.454 & 9.297 \\
fla-qhid-360-1.cnf & 39.998 & 39.797 \\
fla-qhid-360-5.cnf & 10.698 & 10.849 \\
fla-qhid-380-1.cnf & 189.895 & 249.003 \\
%fla-qhid-400-3.cnf & 83.098 & 130.834 \\
fla-qhid-400-4.cnf & 55.213 & 52.811 \\
smallSatBomb.cnf & 0.011 & 0.022 \\
\end{tabular}
\caption[Benefit of keeping learned clauses in $IBB$]{Backbone computation time of the $IBB$ algorithm, once with keeping learned clauses ($t_{keep}$) and once discarding learned clauses between every SAT call($t_{discard}$)}
\label{tab:learnedIbb} %for referencing
\end{wraptable} 
Taking a look at this table, we can quickly see that for these instances, all solvers that employed preferences as part of their algorithm performed much worse than those that did not. These would be $BB$,$IBB$ and $BB_{unit}$. This impression is further supported by the performance results of the three configurations of the $PB2$ algorithm. Here the effect of preferences was reduced incrementally, eventually reaching a computation time close to that of $IBB$ where no preferences were set.

%You can also see a vague inverse correlation between the number of sat calls and the calculation time. wenige sat calls heissen nicht automatische wenig rechenzeit, womöglich aber auch nur weil hier eine schwierige datei ist.

%TODO IBB hatte bug, darum bessere leistung von pb1x vgl IBB\\
%zweite tabelle zeigen, wenn blocking clause nicht entfernt wurde

\subsection{Importance of reusing learned clauses}
Table \ref{tab:learnedIbb} shows a comparison of individual benchmarks to highlight the importance of reusing learned clauses. While working with the $Sat4J$ library I noticed that at that point in time the $IBB$ backbone algorithm was accidentally configured in a way that learned clauses would always be discarded between model computations. However, these learned clauses are still valid in later iterations of the $IBB$ algorithm. The only difference that the formula goes through during this algorithm, is that the blocking clause that ensures a new model repeatedly looses some of its literals. As long as the set of solutions for a formula is only reduced, the learned clauses of that formula stay valid.
%\footnote{Example: With a formula without any clauses but three variables $a$,$b$ and $c$ you can create eight models. With only the clause $\{a\lor b \lor c\}$ in your formula you can have seven models, with only the clause $\{a \lor b\}$ you can have six models, and with only the clause $\{a\}$ in your formula you can have four. The blocking clause of $IBB$ would shrink in the same order as in the previous sentence and similarly, for each listed formula, its models would also occur in the formula before it.}

The information contained in learned clauses is very valuable, as it prevents the solver from repeating invalid combinations of assignments that might even be likely to occur again. But if already learned clauses can guide the SAT solver away from possible conflicts, it could ideally return a model without any backtracking.

\subsection{Benefits of unit implication}
\label{ss:result_unit}
This section evaluates the effects of trying to recognize backbone literals through unit implication as described in section \ref{subsec:unitImpl}. Table \ref{tab:satCompAvg} interestingly shows no performance benefit for this technique, even though the number of SAT calls is much smaller.



\begin{table} %[tbp]
\begin{tabular}{l| c c c c }
File & $t_{BB}[n_{sat}]$ & $t_{BB_{unit}}(t_{sat})[n_{sat}]$ & $n_{unit}$ & $n_{backbone}$  \\
\hline
brock400-2.cnf & 0.04[254] & 0.04(0.03)[254] & 0 & 0 \\
fla-komb-400-3.cnf & 1276.71[381] & 1241.12(1241.11)[45] & 336 & 379 \\
dimacs-hanoi5.cnf & 1.94[1,931] & 1.84(1.23)[281] & 1,650 & 1,931 \\
vonThore42.cnf & 0.02[398] & 0.01(0.01)[51] & 345 & 346 \\
grieu-vmpc-s05-27.cnf & 261.09[678] & 281.08(277.1)[536] & 142 & 677 \\
fla-komb-360-4.cnf & 14.94[337] & 14.94(14.9)[45] & 292 & 333 \\
fla-qhid-360-1.cnf & 74.96[355] & 76.03(76.03)[29] & 326 & 355 \\
grieu-vmpc-s05-25.cnf & 182.84[625] & 179.91(179.44)[51] & 574 & 625 \\
fla-qhid-360-5.cnf & 20.68[350] & 20.89(20.89)[29] & 321 & 349 \\
fla-barthel-220-4.cnf & 3.55[32] & 2.74(2.73)[32] & 0 & 6 \\
9012345.cnf & 4.18[1483] & 6.73(5.92)[813] & 670 & 1,478 \\
fla-barthel-220-2.cnf & 8.18[23] & 8.11(8.11)[23] & 0 & 4 \\
1098765.cnf & 1.21[938] & 1.29(0.83)[493] & 444 & 921 \\
smallSatBomb.cnf & 0.01[26] & 0.01(0.01)[19] & 7 & 9 \\
\end{tabular}
\caption[Benefits of unit implication]{Benchmark results for a selection of files with a focus on the benefit of unit implication.
Rows indicate: Calculation time and number of SAT calls for the $BB$ solver ; Calculation time, time spent in SAT solver and number of SAT calls for the $BB_{unit}$ solver ; Number of backbone literals identified through unit implication (in $BB_{unit}$) ; Number of backbone variables in formula.}
\label{tab:bbkbb}
\end{table}

Table \ref{tab:bbkbb} shows individual performance differences between the $BB$ solver supplied by $Sat4J$ and my own modification $BB_{unit}$ for comparison, as well as the backbone's size and the number of backbone literals identified through unit implication. The time columns also contain the number of SAT calls that were comitted. Its content matches the results of table \ref{tab:satCompAvg}. Occasional cases where $BB_{unit}$ trumps in performance over $BB$ get balanced out by cases where this is the other way round, but the number of SAT calls is always better with the $BB_{unit}$ algorithm. And this is true in spite of the fact that almost all identifications happened through unit implication.\footnote{
	In table \ref{tab:bbkbb} the pure SAT calculation time for the $BB$ column is missing. The $BB$ algorithm actually spends almost no time outside of the SAT solver in the case of the listed formulae.
	%\\Most of the performance differences in this table can be explained with other reasons than actual algorithmic differences. $fla-barthel-220-4.cnf$ for example should not be faster with $KBB$ since here no backbone literal was determined through unit propagation. However I was able to reproduce this performance difference just through the order of what was called first, i.e. if the two solvers are called the other way around the difference in timing between the first and second call are the same. A possible explanation could be a power saving feature in modern processors.
}

This indicates that the effect from scanning for unit implied backbone literals also appears in the $BB$ solver, only not quite as obvious. When you look at line \ref{algo:bb:learn} of algorithm \ref{alg:bb}, you see that $BB$ actually takes identified backbone literals up into the formula to speed up the solving process\footnote{Just like any other method by reusing learned clauses}. Now imagine what happens, if the unit implication case happens during the course of the $BB$ algorithm. You have a clause that is not satisfied, even though all but one of its literals are already assigned through the learned backbone literals. When $BB$ tests the last of that clauses literals with an assumption against it, the clause becomes immediately unsatisfied\footnote{Remember that $BB$ tests by trying to disprove a potential backbone literal.}, proving that the last remaining literal in the clause is part of the backbone. This happens without a single decision or conflict. 

The results shown in the following chapter (\ref{sec:sectionVonThore}) paint a slightly different picture. Here the pure SAT time of $BB_{unit}$ is actually smaller than that of $BB$, however even $BB$ looses around a third of its calculation time outside of pure SAT calls. This could be explained by the much higher number of SAT calls for this benchmark and the overhead that comes with preparations and cleanups for a SAT call, which appearantly became relevant at that point. Given good conditions (like in that benchmark) $BB_{unit}$ can identify many backbone literals in a single search, which also requires less data structures around it compared to a complete SAT computation.

In summary, searching for backbone literals through unit implication can be a more efficient strategy than testing each individual literal through a SAT call, however only if the search for unit literals is implemented in a very efficient way. %Also note that the problem of having too many SAT calls can just as well be resolved by using an enumeration algorithm like $IBB$ instead of an iterative approach.

\subsection{Effect of subset preferences}

The algorithms where the set of preferences was restricted in size ($PB1c$ and $PB1d$) compared relatively bad to the base algorithm $PB1$ and this is most pronounced in the case of $PB1c_{5\%}$ where the restriction is the strongest. This means that, at least for this benchmark, restricting the number of preferences completely backfired. The decrease in performance can be traced beginning with $PB1$ which is equivalent to $PB1c_{100\%}$ towards the $50\%$ and $5\%$ configuration.

%This behaviour matches an observation that you can make with the duration of the last SAT call. All of these take longer than the average. The commonality with the subset preferences is that the set of preferences is small here as well, since it 

For a possible explanation I should begin with a reminder about the exact way in which preferences are implemented in $PB1$. Here two decision heuristics coexist, $h_{pref}$ and $h_{tail}$, whereas under normal circumstances a $CDCL$ SAT solver would only use one to pick a literal for a decision. In $PB1$, $h_{pref}$ is always consulted first, and only if all the variables that it offers for an assignment are already assigned to a value, $h_{tail}$ is queried. Both $h_{pref}$ and $h_{tail}$ contain a heuristic to choose the optimal variable for a decision, but in $h_{pref}$ the set to choose from is restricted and what that variable would then be assigned to, is fixed. In contrast, $h_{tail}$ is free to choose any remaining free variable and give it either boolean value, depending on what it deems better to satisfy the formula.

When a decided literal is involved with a conflict, it will be pushed back in its decision heuristic. That way an opportunity is given to variables that might not be so difficult to be assigned a good value and the problematic variables might be assigned a necessary value through unit implication. 
%This could even cancel out an unsatisfiable preference that would conflict with a necessary assignment (i.e. a backbone literal). 
However this can only happen if enough other variables are available in the same heuristic to take its place and with its strong size restriction, $PB1c(5\%)$ doesn't have them.

Additionally, the more that you restrict the size of the preferences, the more likely it becomes that all preferred literals are negated backbone literals. In this case, all decisions made based on $h_{pref}$ prevent you from finding a model and must eventually be reverted.

Even worse, since the preferred assignments have to be done before all the others, they occur at the beginning of the CDCL table. This means that in case of a conflict with other assignments, the preferred decisions will not be reverted if there is any other decision available to be reverted, since the strategy of CDCL is to revert only the youngest decision involved with the conflict. But if this preference actually goes against a backbone literal of the formula, it must eventually be reverted to reach a valid model. This can only happen by a unit propagation, since otherwise a decision would just take the preference into account again. And since no decision can happen before the preferred decisions, to counter a preference a unit propagation requires the maximum amount of information so that it can happen without a prior decision. Not only do you need to identify a variable assignment as part of the backbone, but before that you have to learn enough other clauses so that you can reach the decisions that were made from unsatisfiable elements in $h_{pref}$.

%eigentlihc: dass anstatt schlecht funktionierenden präferenzen bessere benutzt werden

%However, this extreme amount of work could potentially be made use of, by combining strategies from $PB0$ and $PB1c$. The strategy of $PB0$ incorporates to check every time, whether any of it's preferences could be implemented. If not it is interpreted as that each preference goes against a backbone literal. The same can be applied to $PB1c$ to iteratively identify multiple backbone literals in case they all could not be implemented wahrschienlich nicht, kp wie das mit den blocking clauses interagiert.


%The big difference between typical $PB1$ variants and $PB1c$ is that this problem of having no satisfiable preferences not only applies to the very last SAT call, but also to many other ones. This can easily happen if the chosen subset consists only of literals that go against the backbone, multiplying the number of problematic SAT calls. 


\subsection{Duration of last SAT call}
\label{ss:durLastCall}

\begin{wraptable}{r}{5cm}
\center
\begin{tabular}{l| c }
 & $t_{last}/t_{avg}$  \\
\hline
$IBB$ & 3.479 \\
$PB0$ & 0.147 \\
$PB1$ & 2.000 \\
$PB1_{model}$ & 1.682 \\
$PB1_{rotate}$ & 1.703 \\
$PB1_{forget}$ & 2.640 \\
%$PB1x$ & 2.777 \\
%$PB1a$ & 3.808 \\
$PB1b$ & 1.178 \\
$PB1c_{50\%}$ & 2.550 \\
$PB1c_{5\%}$ & 4.495 \\
$PB1d_{50\%}$ & 2.109 \\
$PB1e$ & 2.457 \\
%$PB1f$ & 1.627 \\
$PB2_{50\%}$ & 6.343 \\
$PB2_{5\%}$ & 2.989 \\
$PB2_{0.5\%}$ & 3.113 \\
\end{tabular}
\caption[Comparative duration of last SAT call]{Ratio between the average duration of a SAT call and the duration of the last SAT call. Calculation is based on the values in table \ref{tab:satCompAvg}}
\label{tab:lastTimeToAvgTime}
\end{wraptable}
A similar effect to the previous is hinted by the the third column $t_{last}$ of table \ref{tab:satCompAvg}. Many of these timings are longer than the average computation would take, which is easier to recognize in table \ref{tab:lastTimeToAvgTime}. The observation coincides exactly with whether the last SAT call would be unsatisfiable. The $PB0$ solver does not apply blocking clauses and still performs well in the last iteration, whereas the $IBB$ solver that does not employ preferences performs badly\footnote{
	The $BB$ solvers are excluded from this table, because they are iterative algorithms. Algorithmically the first and last SAT calls do not differ that much.
}.This indicates that the problem lies in the fact, that in this last SAT call, the formula is unsatisfiable.

That $PB0$ performs so well in the last SAT call is actually quite interesting, as here the last SAT call is about seven times faster than its own average. This could be explained by the fact that since the formula is never really changed in the case of $PB0$, the clauses that it learns never become outdated. In contrast, in the typical model enumeration schemes, they may not become invalid but at least less useful over time, because they might only affect the generation of implicants that are later excluded from the formula through a stronger blocking clause. The information stored in these learned clauses can be put to use especially well in the last iteration. Theoretically, the preferences at that point in time are all impossible to implement and should slow down that particular calculation. 
But the solver has already spent a lot of time with conflicts that stem from them and should thus have learned many clauses that involve the backbone literals, possibly even directly identifying them as such. In that particular case, the corresponding preference is overridden immediately and $PB0$ merely needs to get the remainder of the formula to satisfy, just like any other backbone algorithm.


%TODO alte these ausgraben: backbone literale stehen in CDCL tabelle vor erster decision, beweisen, oder einfach weg


\begin{wraptable}[14]{r}{7cm} %[tbp]
\begin{tabular}{l| c c }
 & Permanent & Forgetting  \\
\hline
$PB1$ &	20.251 & 12.426 \\
$PB1_{rotate}$ & 21.255 & 13.154 \\
$PB1c_{50\%}$ & 25.708 & 13.109 \\
$PB1c_{5\%}$ & 1080.724 & 14.483 \\
$PB2_{50\%}$ & 13.023 & 7.025 \\
$PB2_{5\%}$ & 6.283 & 6.348 \\
\end{tabular}
\caption[Benefit of forgetting preferences]{Average of the complete backbone computation for variants of $PrefBones$ with and without forgetting preferences.}
\label{tab:satCompForgettingBenefits}
\end{wraptable}


\subsection{Benefits of forgetting preferences}
%Table \ref{tab:satCompAvg} shows a relatively good result for $PB1(amnes)$. It is still worse than the solvers without any preferences, but in table \ref{tab:vonThore1} where pre
Table \ref{tab:satCompForgettingBenefits}
%\footnote{These performance ratings differ slightly from table \ref{tab:satCompAvg}. This is because it is a different execution.}
lists a comparison of multiple backbone algorithms once with ordinary, permanent preferences and once in a forgetting scheme as described in section \ref{sec:amnesPrefs}. Preferences in general resulted in worse performance for this benchmark. However, $PB1_{forget}$, where the preferences are configured not to be applied a second time after they were reverted through conflict resolution, reduced the penalty to an acceptable level in all examples\footnote{
	In earlier executions of my benchmark setup, I also tested $PB1c$ with $0.5\%$ as preference size restriction, however only in combination with forgetting preferences. Without forgetting them, this configuration turned out to be completely unreasonable to test within an acceptable time frame and was dropped.}.


Table \ref{tab:vonThore1} in the upcoming section shows results for a formula that is more beneficient to preferences. Here we see, that in such a case, forgetting preferences still work as intended. If these results turn out to be reproducible for other formulae as well, forgetting preferences could be a viable strategy to compute the backbone of any formula without prior knowledge about it, since the penalty for difficult formulae would be relatively low but the speedup for easy ones very high.


%in second benchmark sogar besser als PB1, hat möglicherweise schwierige und leichte komponenten und amnes kann mit beiden gut umgehen.