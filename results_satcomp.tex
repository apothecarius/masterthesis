\section{SAT Competition Benchmark}

For the first benchmark, I collected a set of 71 files from a SAT competition TODO welcher. The SAT competitions generally use problems that are difficult to solve compared to other problems of the same file size. This is in order to encourage development of solving strategies that reliably have good performance and not only for most of them. To save time during benchmarking, those files that took longer than around one minute for a single model computation were excluded, resulting in files that are generally around 30 kilobytes large. Additionally, problems where the duration for backbone computation averaged below one second were excluded, because here the small differences in the measurements could just as well be explained with external factors like the CPU throttling for a short time. To get meaningful testresults for such files, multiple testpasses should be conducted.

\begin{table} %[tbp]
\begin{tabular}{l| c c c }
 & $t_{full}$ & $t_{sat}$ & $n_{sat}$ \\
 \hline
BB & 25.767 & 25.765 & 275 \\
IBB & 27.086 & 27.07 & 7 \\
KBB & 25.454 & 25.29 & 42 \\
KBB(pref) & 54.729 & 54.619 & 39 \\
PB0 & 57.177 & 57.175 & 5 \\
PB1 & 131.063 & 131.061 & 5 \\
PB1(noAmnes) & 171.183 & 171.182 & 4 \\
PB1(model) & 116.853 & 116.852 & 6 \\
PB1(invert) & 164.994 & 164.992 & 11 \\
PB1x & 18.564 & 18.562 & 8 \\
PB1a & 154.091 & 154.089 & 7 \\
PB1b & 144.028 & 144.026 & 6 \\
PB1c(50\%) & 119.571 & 119.569 & 6 \\
PB1c(5\%) & 141.334 & 141.332 & 6 \\
PB1d(50\%) & 131.602 & 131.6 & 6 \\
PB1d(5\%) & 148.734 & 148.732 & 6 \\
PB1e & 143.675 & 142.621 & 6 \\
PB1f & 224.335 & 224.334 & 6 \\
PB2(50\%) & 228.267 & 228.264 & 8 \\
PB2(5\%) & 182.26 & 182.258 & 8 \\
PB2(0.5\%) & 165.892 & 165.89 & 8 \\
PB3 & 157.297 & 157.249 & 5
\end{tabular}
\caption{Averages of 71 testfiles taken from sat competitions. The columns indicate: The full time that the calculation took in seconds; The time that was spent in the sat solver; The number of sat calls (all three values are averages). }
\label{tab:satCompAvg} %for referencing
\end{table}

The averaged time to compute the backbones of these 71 testfiles can be seen in \ref{tab:satCompAvg}. To prove that various computation strategies are implemented as good as possible\footnote{e.g. no noteworthy timeloss during setup of preferences}, the second column shows the time that was only spent in the sat solver.

Taking a look at this table, we can quickly see that for these instances, all solver that employed preferences as part of their algorithm performed much worth than those that did not\footnote{Which would be $BB$,$IBB$,$KBB$ and $PB1x$}.

IBB hatte bug, darum bessere leistung von pb1x vgl IBB\\
zweite tabelle zeigen, wenn blocking clause nicht entfernt wurde


\subsection{Benefits of unit implication}

This subsection evaluates the effects of trying to recognize backbone literals through unit implication as described in section \ref{subsec:unitImpl}. Table \ref{tab:bbkbb} shows individual performance differences between the $BB$ solver supplied by Sat4J and my $KBB$ solver for comparison, as well as the backbone's size and the number of backbone literals identified through unit implication. The time columns also contain the number of sat calls that were comitted.\footnote{Most of the performance differences in table \ref{tab:bbkbb} can be explained with other reasons than actual algorithmic differences. $fla-barthel-220-4.cnf$ for example should not be faster with $KBB$ since here no backbone literal was determined through unit propagation. However I was able to reproduce this performance difference just through the order of what was called first, i.e. if the two solvers are called the other way around the timings are so as well. A possible explanation could be a power saving feature in modern processors.}

\begin{table}[h!] %[tbp]
\begin{tabular}{l| c c c c }
File & $t_{BB}(n_{sat})$ & $t_{KBB}(n_{sat})$ & $n_{unit}$ & $n_{backbone}$  \\
\hline
brock400-2.cnf & 0.041(254) & 0.038(254) & 0 & 0 \\
fla-komb-400-3.cnf & 1276.712(381) & 1241.124(45) & 336 & 379 \\
dimacs-hanoi5.cnf & 1.944(1931) & 1.836(281) & 1650 & 1931 \\
vonThore42.cnf & 0.018(398) & 0.014(51) & 345 & 346 \\
grieu-vmpc-s05-27.cnf & 261.091(678) & 281.083(536) & 142 & 677 \\
fla-komb-360-4.cnf & 14.937(337) & 14.936(45) & 292 & 333 \\
fla-qhid-360-1.cnf & 74.964(355) & 76.032(29) & 326 & 355 \\
grieu-vmpc-s05-25.cnf & 182.837(625) & 179.914(51) & 574 & 625 \\
fla-qhid-360-5.cnf & 20.678(350) & 20.892(29) & 321 & 349 \\
fla-barthel-220-4.cnf & 3.553(32) & 2.741(32) & 0 & 6 \\
9012345.cnf & 4.181(1483) & 6.734(813) & 670 & 1478 \\
fla-barthel-220-2.cnf & 8.183(23) & 8.109(23) & 0 & 4 \\
1098765.cnf & 1.208(938) & 1.291(493) & 444 & 921 \\
smallSatBomb.cnf & 0.013(26) & 0.012(19) & 7 & 9 \\
\end{tabular}
\caption{Benchmark results for a selection of files with a focus on the benefit of unit implication.
Rows indicate: The number of backbone literals identified through unit implication; the actual number of backbone literals; Calculationtime and number of sat calls of the solvers $KBB$ (using unit implication) and $BB$ (not doing unit implication, otherwise the same).}
%\end{wraptable}
\label{tab:bbkbb} %for referencing
\end{table}



The results drunter und drüber, auf einzeldateien eingehen

Begründungsvermutung: Hauptzeit geht für wenige schwierige Literale drauf, die nicht per unit implikation gefunden werden können. Könnte experimentell festgestellt werden, dur

%brock dateigröße: 500kb, fla dateigrößen ~30kb


reine sat zeit von kbb wurde auch gemessen: verlust durch suche liegt bei 0

TODO: BB mit KBB ohne implikationen prüfen, ob benefit durch schlechte implementierung verloren ging
