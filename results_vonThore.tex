\section{Industrial benchmark}

%\end{table}

\begin{wraptable}{R}{8cm}
%\begin{table}[tbp]
\begin{tabular}{l| c c c }
&	$t_{calc}$(sec) &	$t_{sat}$(sec)	& NSatCalls \\
\hline
BB & 11.117 & 7.235 & 159545 \\
IBB & 6.353 & 2.316 & 15748 \\
KBB & 13.747 & 2.11 & 15266 \\
PB0 & 3.659 & 1.469 & 6531 \\
PB1 & 3.918 & 1.611 & 6531 \\
PB1(amnes) & 4.067 & 1.682 & 6531 \\
PB1(model) & 2.148 & 1.59 & 6531 \\
%PB1(invert) & 8.154 & 3.26 & 15248 \\
PB1x & 7.596 & 2.94 & 15748 \\
PB1a & 5.096 & 2.112 & 9335 \\
PB1b & 7.754 & 3.199 & 15248 \\
PB1c & 5.376 & 2.248 & 9680 \\
PB1c(5\%) & 7.026 & 2.693 & 13955 \\
PB1d & 4.464 & 1.804 & 7386 \\
PB1e & 3.524 & 1.354 & 6387 \\
PB1f & 3.603 & 1.526 & 6369 \\
PB2 & 8.341 & 3.457 & 15752 \\
PB2(5\%) & 8.313 & 3.484 & 15752 \\
PB2(0.5\%) & 8.364 & 3.529 & 15752 \\
PB3 & 3.872 & 1.204 & 4471 \\
\end{tabular}
\caption{First execution of industry application. Values are not averaged, but summed up over 407 different benchmarks.}
\label{tab:vonThore1} %for referencing
\end{wraptable}

Another benchmark I applied on the various backbone algorithms was a formula for a real world application from the automobile industry. The purpose of this formula $F_{conf}$ was to describe a product in the context of options or features available to the customer. Some of these options can be combined, others exclude or require each other. Most of the variables in $F_{conf}$ correspond to a boolean parameter that is set to $\top$ if the associated feature is requested by the customer. If the formula would become unsatisfiable under such assumptions equal to the requested configuration, then the combination of these features would be impossible. $F_{conf}$ further contains a small set of additional variables to model more complex relationships between options.

The particular use case that I examined was to find the implications in the formula, i.e. if a customer requests feature $a$, would he also have to pick feature $b$. A primitive way to calculate this would be to iterate over all possible pairs of features 
%\footnote{Minus permutations and pairs with the same feature twice}
and check for satisfiability of $F_{conf}$ under the condition that $a$ is $\top$ and $b$ is $\bot$. If this was unsatisfiable, then $a$ would imply $b$. However, with 407 literals to choose from, you would have to do 165,649 sat calls. 

A more efficient approach is to only go over the 407 options once and for each variable $a$ of them calculate the backbone of $F_{conf} \cup \{a\}$ or in other words $F_{conf}$ under the assumption $a$. If a feature $b$ occurs in all models when $a$ is assumed, then $a$ implies $b$ in some way.

Table \ref{tab:vonThore1} lists experimental results with the same set of algorithms that were run on the files from the sat competition. After analyzing performance results, I try to give a specialized algorithm that is optimized for this particular benchmark.

We immediately see, that preferences give a much greater benefit than in the benchmark with the files from the sat competition. However there are some more things that stand out:
\begin{itemize}
\item The fastest algorithm overall is the one that does not reduce the models at all. The number of sat calls is even exactly the same as that of that variant with prime implicant reduction. However the time that was spent in the sat solver is very similar to that execution with prime implicant reduction ($PB1$), so the difference must be the time that was spent to reduce the model and the benefit, namely the number of optional literals in the prime implicant, was very small.
\item However, the fastest implementation when it comes to pure sat solver time as well as number of sat calls, was $PB3$, which differs from the others in that it uses literal rotation to reduce models instead of prime implicant computation. Apparantly the models that occur in this formula contain many different prime implicants with little distance to the model.\\
A previous variant of this algorithm took around 250 seconds to compute overall. This version did not use the lookup from literal to containing clauses (as described in later paragraphs in section section \ref{ss:rot}), but iterated over all clauses in the formula. Such a drastic effect of an efficient implementation did not occur for the sat competition benchmark.\\
This has primarily two reasons. First, the number of sat calls per instance is twice as much in this benchmark compared to those from the sat competitions\footnote{The 4471 calls span over 407 problem instances.}, therefore the model reduction happens twice as often. Secondly, the formula of this benchmark is much larger than those from the sat competition, with the filesize being around twenty times as large. Therefore there are much more clauses to search through.
\item The two algorithms in third place of overall computation time were $PB1e$ and $PB1f$. Both of these make use of learned literals so appearantly the backbone literals in this formula are easy to identify by checking the set of literals that the solver learns when it computes new models.\\
In fact, on inspection, all literals that later turn out to be part of the backbone occur in this set after the very first sat call(with an exception of the forced literal, which can be extracted by searching the formula for clauses with only one literal).
\item Another noteworthy algorithm is the $KBB$ algorithm that uses unit implication to identify backbone literals. The implementation of this algorithm was based on that of the $BB$ algorithm and here the number of sat calls with $KBB$ is a tenth of the number in the case of $BB$. However this advantage is not expressed equally strong when it comes to the time spent in the sat solver(around a fourth), which means that the identification through calculating a new model can sometimes be more efficient than regularly checking unit implications.\\
The overall computation time is even worse in comparison, meaning that the benefit through cheap backbone literal identification is less than the time that it takes to check the clauses for the unit case. It may be possible that this method requires an improved implementation. The search for clauses where the unit implication for backbone literals applies could be implemented in a similar way as was used to calculate required and rotatable assignments, using the lookup from variables to clauses where they occur (see section \ref{ss:rot})
\end{itemize}

verbesserungen:
- nicht brute force klauseln prüfen, sondern anhand des index für watched literal propagation (notiz: monoclauses werden in sat4j nicht gelernt. Sollten stattdessen eh als assumptions übergeben werden)
%- außerdem: lits in bb_l muss man nicht prüfen ob notwendigerweise in bb_u
- nur die literale rotieren die in der obermenge verbleiben
- Außerdem: "bugfix" erzwungenes literal trat nicht in gelernten auf.
- präferenzen nutzen

besonderheiten: 
- absolut alle backbone literale sind nach dem ersten sat aufruf gelernt. (*mit technischen besonderheiten, nur wenn man blocking clauses nimmt und dann ausgenommen die in der blocking clause)

Line \ref{algo:kbbDoWhileStart} bis Line \ref{algo:kbbDoWhileEnd} do while breche ab sobald nix neues gefunden wird. unitImplied gibt nur neue zurück


\begin{algorithm}
\caption{{\sc Specialized algorithm for industrial application}}
\DontPrintSemicolon
\KwIn{A satisfiable formula $F$ in CNF}
\KwOut{All literals of the backbone of $F$}
$(outc,mdl,learnt) \gets SAT(F)$\;
$bb_u \gets required(mdl,F)$\;
$bb_l \gets learnt$\;
\While{$bb_l \neq bb_u$}{
	\Do{$unitBackbones \neq \emptyset$}{
	\label{algo:kbbDoWhileStart}
		$unitBackbones \gets unitImplied(F,bb_l)$\;
		$bb_l \gets bb_l \cup unitBackbones$\;
	}
	\label{algo:kbbDoWhileEnd}
	$blocker \gets \bigvee _{l\in bb_u}\neg l$\;
	$prefs \gets \{\neg l : l \in bb_u \}$\;
	$(outc,mdl,learnt) \gets prefSAT(F \cup blocker \cup bb_l, prefs)$\;
	\If{$\neg outc$}{
		\Return{$bb_u$}\;
	}
	$bb_l = bb_l \cup learnt$\;
	$bb_u = bb_u \cap required(mdl,F,bb_l)$\;
}
\Return{$bb_u$} %\tcp{Equivalent to $bb_l$}
\label{alg:thoreSpecial}
\end{algorithm}

%über algo:thoreSpecial
%- gelernte klauseln muss man nicht extra hinzufügen, weil gelernte literale gelernt bleiben
%- für $bb_l \neq bb_u$ muss man nur größe vergleichen, weil upper/lower bound desselben ziels
%- 

besondere bedeutung für diesen benchmark: wiederverwendung von gelernten klauseln über unterschiedliche literale